[{"categories":["reviews"],"contents":"After a bit of faffing around with invoicing and reimbursing, I have finally managed to get ahold of a machine specifically for work. At my company\u0026rsquo;s request, I have purchased a MacBook Pro with the Apple M1 chip.\nAnybody who has read my blog knows I have significant misgivings about Apple, their products, and their business practices. I am generally not a fan of working with Macs, but I must confess I was intrigued to see how this new chip would perform.\nFor comparison, I recently bought a Dell XPS 13 for my own personal use. Since both machines are similarly specced ultrabooks, I\u0026rsquo;m using it as my benchmark for these musings.\nmacOS still has a lot of issues My setup on this machine was pretty miserable. Having previously set Windows 10 up in a matter of a couple of minutes and gone straight from registering my biometrics to never being asked for a password again, I was expecting a similar level of polish from Apple.\nThings went wrong almost instantly.\nAfter logging in with my iCloud account and going through the usual \u0026ldquo;do you want to give us extra information?\u0026rdquo; questions, I hit a 20 minute pause as – for seemingly no reason – the computer could not establish a connection with iCloud any more.\nThe endless spinning. pic.twitter.com/NUWV0q1T2C\n\u0026mdash; Ciarán Ainsworth (@thatdapperbrit) April 8, 2021  In the end I had to back out of that step and start again. Not a good start, Apple.\nAs I mentioned in my previous musings on my Dell machine, Microsoft\u0026rsquo;s Windows Hello is a brilliant hack that saves me a lot of time, so when Apple offered to let me use my fingerprint in place of a password I was eager to accept. I knew from my experiences with the iPhone that I would have to use my password on initial logins, but any time saving is good. So, with all this out of the way, I got logged in and set about installing my important apps, starting with my password manager (I had previously been reading my password from my phone, which I cannot recommend).\nWhen I opened up the macOS App Store, I noticed I was not logged in. But how could that be? I had logged in to iCloud as part of the setup process! Without logging in, I was not allowed to download anything, so I grumbled, put in my password again, and logged in. After doing this, I could download apps. Great.\nExcept, wait, no. I could install apps I had previously owned, but trying to install a new application (such as my work\u0026rsquo;s password manager) prompted for a password.\nBut, what? I have now had to authenticate three times in the last 10 minutes! This is absurd! Anyhow, after entering my password one last time, the system prompted me to use my fingerprint and I have not had to deal with this again.\nThat was poor, Apple.\nBesides this, Apple\u0026rsquo;s multi-tasking remains abysmal. Even with the best efforts of apps such as Magnet, macOS simply was not designed to allow for effective multi-tasking. The system is slow to switch between desktops (a process you cannot speed up), window snapping is laughable, and the keyboard shortcuts remain significantly more confusing and awkward than their Windows/Linux/BSD equivalents.\nEnough bitching, what\u0026rsquo;s bitchin'? Okay, so this machine has a lot of really cool things going for it, mostly related to that chip.\nThe MacBook is incredibly snappy. Apart from the delays that Apple has artificially placed in the system because ~aesthetics~, I have had absolutely no stuttering even as I hammer the poor thing with non-optimized apps and electron. This is a huge step up from the last MacBook I used and I have to say I\u0026rsquo;m very impressed. I\u0026rsquo;m currently writing this in VS Code which is noticeably quicker on my Mac than on my PC.\nIt\u0026rsquo;s also cool. My PC will heat up pretty sharpish as I\u0026rsquo;m using more and more (and I\u0026rsquo;ve also come across an issue where shutting the lid will put it into a death spiral of overheating for… some reason), but the MacBook has no such issues. I\u0026rsquo;ve been using the thing all day and it is still cool to the touch.\nAnd I do mean all day. I started working on this thing 13 hours ago and I\u0026rsquo;m just now starting to dip below 20% battery. That\u0026rsquo;s crazy. My Dell will get a solid 5 hours of heavy use out of it but no more. This bad boy gives no shits about a whole day of work. It\u0026rsquo;s the first machine I\u0026rsquo;ve used that hasn\u0026rsquo;t felt like I\u0026rsquo;m being bullshitted when battery stats are quoted.\nShout out to a feature I used to hate When the MacBook Pro first sported a touch bar, I scoffed, I\u0026rsquo;ll admit. I was dismayed at the removal of the fn keys and thought that a touch bar was a terrible replacement. I\u0026rsquo;m happy to say those fears have been rendered unfounded. The touch bar feels like a natural extension of everything you do on the machine, and several apps have some very clever contextual uses for it. In particular, I like that typing a command into the terminal will pop up with the option to bring up its man page. That\u0026rsquo;s a really nice little thing that makes working with the computer just that much nicer. I think you could still have physical function keys and the touch bar, but I\u0026rsquo;m not going to lose any sleep over it.\nOverall This is a very impressive machine, and I can see exactly why it\u0026rsquo;s topped so many for best laptop. I\u0026rsquo;m afraid macOS is far from ready to be my daily driver, but if this is a sign of things to come for ARM, I\u0026rsquo;m hyped. With the recent announcement that ARM will be a tier 1 target for FreeBSD, plus the work that I\u0026rsquo;ve been seeing in the Windows and Linux world to get better ARM support, the future has never been brighter for RISC. The M1 Mac line proves that there is very much a premium experience to be had with ARM processors.\nI can\u0026rsquo;t wait.\n","permalink":"https://sporiff.github.io/posts/m1-mac/","tags":["apple","mac","m1"],"title":"Working with the M1 mac"},{"categories":["blog"],"contents":"I\u0026rsquo;ve wanted to live abroad since I was a very young child. As a beady-eyed youngster I distinctly remember telling my parents I was going to be a vet out in Japan working at the base of mount Fuji. Obviously, at that age I had no idea the logistical challenges such a move would bring. Even if I were to study veterinary medicine and Japanese simultaneously, I very much doubt there would be any appetite for a practice opening at the base of a mountain. Bit of a flight of fancy, but the point remains I knew I wanted to get out of the UK.\nThis feeling has only intensified as I\u0026rsquo;ve got older. I\u0026rsquo;ve travelled around and studied in Asia, and have vacationed in many places across Europe. Every time I plod my way back to the soggy, grey drear of the UK with a heavy heart and a strong desire to leave. Familiarity certainly does breed contempt, and the country\u0026rsquo;s apparent need to throw away all ties to the world have only deepened my sadness at its trajectory.\nSo, with the UK cutting itself off from the world and with a pandemic raging outside my door, I decided to take the plunge and start looking for work abroad. Seems crazy in hindsight, but somehow it has actually paid off.\nWhy now? Brexit.\nQuite simply, Brexit was one of the biggest determining factors in all of this. I have been dreading the end to my freedom of movement since 2016 as I see it as an essential part of who I am. My heritage is French and Irish, and all my life I have been a free moving European citizen. I am not ready to lose this, and if it means I have to pick between living in Europe and living in the UK, I\u0026rsquo;m afraid the UK is going to lose every time. It simply has less to offer.\nOn a similar note, the technology industry in the UK has taken a big hit. We\u0026rsquo;ve already lost a lot of talent to the EU post-2016 and several companies have actually taken themselves over to the more stable and connected countries of the EU such as the Netherlands and Germany. As somebody whose technology career only started in 2016, I knew I had to follow the jobs out of the country.\nWhere do I start? The hardest part of any of this was actually making the decision to go for it. After all, I had a stable job and a place to live in the middle of a pandemic. But actually, this played into my hands. The pandemic meant that I knew only companies that could work remotely would be looking to hire from abroad, at least for the short term, so I would have time to get to grips with the job before needing to move abroad. Once I started looking around, I saw that the EU\u0026rsquo;s market was generally a lot more open to remote work than the UK\u0026rsquo;s, so it wasn\u0026rsquo;t going to be an issue.\nThe second thing I had to consider was exactly what I wanted to do. When I first started in IT, I was a support agent working with Windows. I then moved into Windows systems management, then to second line support for a software company, then to integrations development. The only thing I knew for a fact I never wanted to do again was helpdesk support, but besides that I didn\u0026rsquo;t really know what I\u0026rsquo;d be adequately qualified for. After some thinking, I decided to shoot for a technical writing job since my education is in English and my professional experience is technical. I\u0026rsquo;m surely a catch, right?\nWhat\u0026rsquo;s in a resumé? One thing you need to understand about working in the UK is that if you don\u0026rsquo;t already have a job title, you\u0026rsquo;re unlikely to be considered for it elsewhere. I\u0026rsquo;ve never officially been a technical writer, and because of this I\u0026rsquo;ve never even been considered for work as a technical writer in the UK. I have a master\u0026rsquo;s degree in English and experience working in a number of technical roles, but because my degree isn\u0026rsquo;t explicitly in technical communications and my experience doesn\u0026rsquo;t carry the title \u0026ldquo;Technical Writer\u0026rdquo;, I may as well not bother. I\u0026rsquo;ve mentioned several times while working on hiring boards that what is written on the resumé is largely irrelevant, and that a person should be able to prove themselves if they\u0026rsquo;re trying to switch paths. Alas, there is still a faction over here that cling to this old world ideal.\nWhen applying in Germany, however, this never seemed to be an impediment. I was getting invites to interviews on the same resumé that had failed me in the UK, and was being asked to prove myself as part of the interview process. What a concept!\nCompensation Money is not really of the highest importance to me as long as I can live comfortably, and I knew that moving to Germany was going to incur a lot of up-front cost and high taxation, but I had already made my peace with that (as long as the public transport actually runs on time, unlike in the UK). However, the remuneration on offer generally seemed a lot fairer than what I was being offered in the UK. It wasn\u0026rsquo;t a knockout difference, but it was enough that the increased taxation wasn\u0026rsquo;t going to diminish it to the same level as what I was earning. I will be earning more as a technical writer than I was as a developer, and that\u0026rsquo;s not something to be sniffed at.\nThe cost of living in Berlin, it turns out, is not significantly higher than Exeter, where I currently live. This means that with the pay increase (even after tax) I\u0026rsquo;m going to be better off living in a major European city than I was in a minor British one. Unfortunately, I won\u0026rsquo;t have a low-rent situation like I do when living with my parents. Boo.\nRelocation, relocation, relocation One of the reasons I accepted the job I did was their relocation package. The hardest part of all of this is going to be the visa application and physical relocation (visas wouldn\u0026rsquo;t have been an issue before Brexit, so… thanks?), but the company has offers in place to lessen the financial and administrative burdens of moving. Very welcome. I\u0026rsquo;m currently in the process of trying to apply for permanent residency, which is challenging. At least the immigration company is doing a lot of the heavy lifting for me.\nThere is going to be a hell of a lot of work involved in now physically hauling myself out of the UK, but I feel like I have a good infrastructure behind me. I\u0026rsquo;ll be starting my new job remotely next week, so hopefully by the time I leave I\u0026rsquo;ll be confident in what I\u0026rsquo;m doing.\n","permalink":"https://sporiff.github.io/posts/the-hunt/","tags":["jobs","work","travel"],"title":"The Hunt"},{"categories":["reviews"],"contents":"So I decided finally (finally!) that I was going to invest in a new device in preparation for my upcoming move to Germany. I\u0026rsquo;ve never really been much of a hardware person, if I\u0026rsquo;m honest. Hardware has always just been a means to an end to get me working with software as efficiently as possible be it productivity software, games, or media software. For this reason my computers have always been a bit off the mark; I will nearly always pick something over-specced or under-specced, something too big or something too small, etc.\nWhatever the case, I owned my last desktop computer back in university (until its liquid cooling unit burst and flooded the motherboard 🙃). My experience with it was that it essentially anchored me to a desk, and I would always need to have a more portable device around to do anything elsewhere. So, for my next major investment I decided to plump for a decently powerful Lenovo Ideapad u410, with a dedicated Nvidia card for gaming.\nBig mistake. One you\u0026rsquo;d think I\u0026rsquo;d remember. In particular, discrete graphics cards are a nightmare to manage on Linux (my primary OS at the time), so I spent a huge amount of time trying to figure out why the battery would drain in just under two hours. Predictably, the gaming performance was… well, bad. I could play certain games but never at a high quality or a good framerate. Bum gravy.\nSo next, thinks I, I\u0026rsquo;ll build a laptop with a really powerful GPU and a really big chassis. That way it should be able to handle these pesky games while still being a portable device right? Well, no. Not at all. The GPU was definitely a big step up from the old machine but the laptop was too big and heavy to be carted anywhere, so for the last four years on the desk it has sat, just a poor man\u0026rsquo;s replacement for a desktop computer, always charging and never discharging. Always ready to play games but never powerful enough to do so satisfactorily.\nSo something has to change.\nThe criteria My biggest gripe with my old computer was its shear heft.\nIts enormous size and weight (not to mention the weight of the power brick it required) was simply not practical for the purposes of travel. While having a powerful machine capable of handling most anything you can throw at it is good, it is not worth the price of your back.\nMy wishlist looked a little like the following when I started out:\n Lightweight Relatively powerful (good enough to play PS2 era games since gaming stopped improving there, let\u0026rsquo;s be honest) Compact A good battery life No discrete GPU Good screen, but not 4k or above (for battery reasons) Premium feel (all of my earlier laptops had been somewhat cheap and I ended up paying for it with plastic splinters) Not a mac Not a 2-in-1 (the hinges will break)  I wasn\u0026rsquo;t entirely sure about my budget, but since my dad was going to be buying my old machine off me and my company had bought back most of the leave I didn\u0026rsquo;t end up taking, I had a little more to play with than previously.\nThe competition The XPS 13 was always a contender from the start. It consistently topped the list of modern-day ultrabooks and got pretty solid reviews across the board. The new M1-powered macs came out ahead a lot of the time but quite frankly I don\u0026rsquo;t believe the hype (I love ARM but let\u0026rsquo;s not kid ourselves here, we\u0026rsquo;re a ways off). Besides these two, a couple of others would pop their heads up occasionally such as the HP Spectre, a few Yogas, and the XPS 13\u0026rsquo;s big brother, the XPS 15. The 15\u0026quot; display seemed a bit overkill for the price hike and since portability was more important to me than screen real estate I opted to focus on the 13 for the XPS line. The Yogas and Spectre both dropped to the bottom of the list due to their transforming and touchscreen nature.\nIn all honesty, the only other machine that even came close to beating out the XPS was the Microsoft Surface Pro 7. I used to use a Surface Pro for work and remember heartily enjoying its 3:2 screen and excellent peripherals. However, sitting with one is a miserable experience. They\u0026rsquo;re great machines to use at a desk or as a tablet, but as a laptop they fall significantly short. Also, we\u0026rsquo;re likely to see a redesign soon-ish so if I were to buy one I\u0026rsquo;d hang on. Ditto for the Surface Book, which looked a better laptop but a significantly less interesting tablet.\nOn paper So on paper what does the XPS 13 have to work with? Well, in honesty I\u0026rsquo;m not really too au fait with spec sheets, so a lot of this is just numbers to me:\n 16GB DDR4 RAM (honestly the bare minimum needed for any semi-serious workload these days) 11th Gen Intel Core i7 Processor 4ghz 512GB NVMe SSD 13\u0026quot; 1920x1200 screen (non-touch) 2x USB-C ports MicroSD slot 3.5mm headphone jack 1,27kg chassis  And that is about it. You can look up the specs in more detail online, but for the purposes of this post this is all I\u0026rsquo;m really going to talk about.\nThe bad Okay, so let\u0026rsquo;s work through this and take stock of the problems. The biggest one is the same as any bloody ultrabook on the market: repairability. If anything happens to this machine at any point I\u0026rsquo;m going to have to return it to the store and get it replaced because every single part is soldered in place. This is the polar opposite of my tiny, underpowered ThinkPad x201s (God bless it) which can be repaired by anyone with a screwdriver and a go-get-em attitude. But such is the trend of the industry, and it\u0026rsquo;s a price you pay for portability.\nThe same goes for the ports. This is a dismal port selection, and one that has already irritated me somewhat after a day of use. Very little in this house connects to USB-C, so I need an adapter for everything. With some foresight I purchased a hub which can handle my USB-A and HDMI peripherals so I have been able to get everything up-and-running pretty easily, but what happens if I am handed a suspicious USB stick on the street? How am I supposed to infect my computer then, hmm?\nIn all honesty I can see that USB-C is the future and I will admit the fast charge is great, but to only have one port free when charging is frankly madness. I am going to endeavour not to leave the machine plugged in all the time to preserve the battery, however, so this might actually keep me on that path. I will say I\u0026rsquo;m glad they kept the 3.5mm jack. Doing away with that would have been a deal breaker.\nApart from that, the machine is fine to me on paper. As I say I\u0026rsquo;m not a hardware buff, so I know that some will scoff at that but it seemed like a reasonable trade-off. The Intel Iris integrated GPU has been reviewed as running older games pretty well, which is all I needed to hear.\nThe good There are a few really nice things about this laptop right off the bat:\n The size: 13\u0026quot; with a bezel-less screen is a really nice combination. The machine is compact but the screen doesn\u0026rsquo;t feel cramped (although I admit I prefer 125% scaling to 150%) The weight. It\u0026rsquo;s 1,27kg for crying out loud The screen resolution is decently high quality, but not so much that it\u0026rsquo;s going to massacre battery life The 11th gen core processor is a significant step up on its predecessors (particularly for graphics, I hear)  None of this is mind blowing in any way, but in all honesty it made the machine come across a really good, solid all-rounder. No fancy peripherals or transformations, no extravagant parts, just a sleek and simple laptop with just enough oomph to get it done.\nMy experience Setup Okay, let me just get this of the way: I had a nightmare setting this machine up and it\u0026rsquo;s not the machine\u0026rsquo;s fault.\nIt\u0026rsquo;s Microsoft\u0026rsquo;s.\nI had bought myself a Windows 10 Professional license key in anticipation of getting this machine (like nearly all others it comes with Windows 10 Home, which I refuse to use because I will not be advertised to after paying a license fee). Stupidly, I\u0026rsquo;d forgotten to check whether or not the key could upgrade an existing edition so when it came to trying to use the key it just refused to take.\n Note to self: write a post on how stupid Windows licensing is\n \u0026ldquo;Never mind,\u0026rdquo; thinks I, \u0026ldquo;I\u0026rsquo;ll just perform a reinstall and put the key in that way.\u0026rdquo; Oh what a foolish child I was. Every single time I would boot into the recovery media using a dongle-connected drive, it would tell me that the SSD could not be loaded due to a missing driver (apparently Microsoft in their infinite wisdom decided not to include it in the recovery ISO). After a bit of digging I tried exporting my device\u0026rsquo;s drivers to load in the recovery media using the following Powershell cmdlet:\nExport-WindowsDriver -Online -Destination E:\\Drivers No dice. These drivers, it complained, were unsigned. So I try downloading direct from the source and unzipping the resulting driver to the drive. Uh-uh, this won\u0026rsquo;t work either.\nTo be honest with you I got so fed up I just bought an upgrade license. It won\u0026rsquo;t hurt to have another activation key hanging around. But I\u0026rsquo;m somewhat absolutely furious because I fought this issue for over an hour. This would never happen with Linux. Linux waits until you\u0026rsquo;ve booted into the live system to give you bad news.\nAnyway, once I got my licensing issue sorted out I set about removing the usual guff that gets installed. To Dell\u0026rsquo;s credit not much was theirs, most was just plain old Microsoft gunk such as that Solitaire game they want me to pay a subscription fee for (lolwot). At this point I turned to winget and started installing my normal packages. Given all my data is backed up on the cloud the actual setup of the machine at this point took only a few minutes.\nPeripherals The USB-C ➡ USB-A adapter packaged with the machine is a sham which drops connection constantly. It caused me no end of blind fury when trying to burn a recovery disk because it simply kept disconnecting. Having now thrown that to the side in favour of my USB-C hub I have found the connections to be stable and reliable. HDMI works well as does the device\u0026rsquo;s fast charging. They weren\u0026rsquo;t lying, you really can get to 80% in an hour. Nothing else to report here really (there\u0026rsquo;s only two ports after all).\nThe sights and the sound I\u0026rsquo;m not going to go so far as to say the XPS 13 has a gorgeous screen – it doesn\u0026rsquo;t hold a candle to the likes of Apple\u0026rsquo;s Retina displays – but it does have good colour quality and brightness. The image is crisp and sharp and text is easily readable, which suits me fine as somebody who works mostly in text. I do find myself missing my larger screen when I\u0026rsquo;m sat away from my desk, but considering the fact that there isn\u0026rsquo;t a 1.200kg behemoth spitting superheated air onto my shattered legs as I\u0026rsquo;m used to I\u0026rsquo;ll put up with it.\nI am, however, shocked at how good the speakers are. I rarely if ever use speakers as I prefer to shut the world out when I\u0026rsquo;m working, but these speakers are phenomenal for a device this tiny. They easily eclipse the volume and quality of any other built in speakers I\u0026rsquo;ve had before, which is a great addition to the overall package.\nUnlimited power? Not quite, but the battery does hold up pretty well. With a few things running and a lot of browser tabs open I can work in my editor for around 6-7 hours without needing to charge, although as mentioned I\u0026rsquo;m trying not to run the battery down so I\u0026rsquo;m not going all the way to 0 or all the way to 100. The battery saver mode is very frugal but the device doesn\u0026rsquo;t grind to a halt as some I\u0026rsquo;ve used have. It\u0026rsquo;s a perfectly comfortable coasting experience.\nThe heft Okay, so this machine is not exactly beefy, but what can one expect in a 1,2kg machine? 16GB of RAM is plenty to get done what I usually do in a day and the CPU is barely registering load most of the time. I can play most of the games I played before including Sauerbraten (a.k.a. the only FPS that matters) at a good framerate, so I\u0026rsquo;m pretty happy with that.\nImportantly this machine runs incredibly quiet. My old laptop roared into live when you hit the button and continued to wheeze and sputter throughout the day which hindered video conferencing something rotten. Unless I\u0026rsquo;m hitting the XPS pretty hard with some taxing task, it sits perfectly quiet and lets me get on with it.\nLet\u0026rsquo;s be honest, I\u0026rsquo;m not putting the machine through any real benchmarks, I\u0026rsquo;m not compiling code or rendering videos like I used to, but for a day-to-day workload it\u0026rsquo;s more than powerful enough.\nThe software Okay, so this isn\u0026rsquo;t really a Dell thing (although Dell has a part to play), but the software on display in Windows 10 has come on hugely since last I used it. I\u0026rsquo;m normally not one for things like biometrics but given that I had a computer with a fingerprint sensor (and given that also I have a great disdain for passwords) I decided to set up Windows Hello. In addition to the fingerprint, I can use the built-in webcam to use facial recognition. This is kind of insane to me. My startup process now goes:\n Open laptop lid, computer boots Sit looking at computer, Hello lets me in Start working  Beginning to end this takes just shy of 9 seconds.\nWHAT\nI have to be honest: in my years of computing I\u0026rsquo;ve never experienced anything that smooth. That is goddamn butter coming straight from the sanded teat of an oiled Greek God smooth.\nThe introduction of winget and a few other productivity tools such as the new Microsoft terminal has made using Windows much closer to using Linux (i.e. it\u0026rsquo;s going the right way). I can now quite happily get on with what I want to do in a day and not have to worry about the weirdness of working on a Windows machine. Git just works, vim just works, the entirety of Linux itself just works. It\u0026rsquo;s astonishing how far we\u0026rsquo;ve come really.\nOverall Thoughts The Dell XPS 13 is a peppy little machine which packs a lot into a very, very small frame. Its (lack of) port selection is very disappointing but not exactly abnormal nowadays, and if you live an always-online life you should be fine with it. It\u0026rsquo;s very comfortable to type on for long periods which makes it ideal for me, and its all-round solid quality makes me think I\u0026rsquo;ll be able to rely on it for a while to come.\n","permalink":"https://sporiff.github.io/posts/xps-13-thoughts/","tags":["computer","hardware","tech","technology","travel"],"title":"Dell XPS 13 thoughts"},{"categories":["blog"],"contents":"Some big life changes The last time I posted anything here was August 2020. Wow. That\u0026rsquo;s a pretty substantial break, but it does also coincide with the last time I had any time off work. So in a way it makes sense.\nA lot has happened since then, but the biggest thing for me personally is that I have taken a job opportunity out in Germany. I\u0026rsquo;ve been trying to get out of the UK for a while now, so to have finally managed to secure an opportunity is pretty huge for me. I\u0026rsquo;m switching careers from programming to technical writing. It strikes me I\u0026rsquo;ve always had more of an interest in explaining how things are achieved than actually achieving them myself, so it\u0026rsquo;s a pretty good fit.\nObviously, the world is in the middle of a plague right now so my physical move isn\u0026rsquo;t going to happen for a little while, but I have officially left my old position and been set up with IT accounts etc. at my new one, so it\u0026rsquo;s all a go from here. I\u0026rsquo;ll be starting my new job remotely on Monday, so I\u0026rsquo;m pretty excited to get stuck in.\nAfter a week off, of course.\nI\u0026rsquo;m incredibly excited about starting a new life in Berlin. The last year has been a particularly tough one for me personally after my relationship broke down and I am feeling the need for change very keenly. I\u0026rsquo;ve also been making efforts to reconnect with old friends with whom I had stopped talking to try and keep myself in the loop. \u0026ldquo;Don\u0026rsquo;t be the asshole\u0026rdquo; is my new mantra.\nSome smaller life changes Of course, with a new life comes new hardware. Since I am switching my focus back to writing I decided it was high time I invest in a normal laptop. You know, one that\u0026rsquo;s actually portable and can sit on your lap. For several years I\u0026rsquo;ve been using a 17\u0026quot; gaming laptop which is neither really a gaming machine nor a laptop as it has the portability and power of a poor desktop. I\u0026rsquo;ve now handed this device to my dad so he has a bigger screen to watch his Chinese dramas on.\nFor my sins, I opted for an ultrabook after much debate. I\u0026rsquo;ve learned my lesson about investing in cheap hardware and decided to go for a top-of-the-line Dell XPS 13. This is a lovely little device and, despite needing a lot of fiddling to get it set up the way I like, I\u0026rsquo;m enjoying it as a portable workstation (however, who the hell decided two USB-C ports was all a laptop needed?).\nHaving fun with Windows In preparation to hand my old machine to my dad, I installed Windows 10 on the device again and have been having a shufti at how the old system is going. From the looks of things, a huge amount has improved. The Windows Store continues to be a colossal waste of my time, but the new package manager has made a huge difference (and did back Keivan Beigi originally made it). I\u0026rsquo;ve even been contributing packages for it on Github just for gits and shiggles. It\u0026rsquo;s great being able to install apps on the command line just like in a proper OS.\nOther than that, Windows continues to be a fairly stable and reliable experience. The machine runs well with it and the terminal functionality combined with WSL mean that I\u0026rsquo;m able to do all of my Linux shenanigans in one place with all the comfort of the world\u0026rsquo;s most widely supported OS.\nGit for Windows still sucks, but what can you do?\nI may write up a full review of the XPS 13 towards the end of the week once I\u0026rsquo;ve had some time to get to grips with it.\n","permalink":"https://sporiff.github.io/posts/finally-change/","tags":["tech","technology","jobs","work","travel"],"title":"Finally, change"},{"categories":["reviews"],"contents":"So you probably already know that Mozilla recently went bang and fired 25% of its workforce before announcing that whoops lol actually we are still going to be able to feed the hungry fox this year. Among the teams affected were the servo developers, a lot of the threat response team, and (I believe) the entirety of the team behind MDN: the only good resource for web development don\u0026rsquo;t @ me. As you can imagine this has led to quite the tizzy as people have puzzled about what exactly this means for the Firefox browser, Mozilla as an organisation, and the future of browsing and, indeed, the web as a whole.\nWhile nothing is likely to change in the immediate future, it seems clear that Mozilla is trying to consolidate its efforts into fewer projects to try and increase short-term stability while sacrificing experimentation. We\u0026rsquo;ve been assured that Firefox is still under active development, and that while Servo is kicking the bucket they are going to continue building new features in Rust for Gecko. But the whole affair has woken the community up to something they\u0026rsquo;ve largely been ignoring for quite a while: Mozilla is effectively the only thing standing between us and a totally Google-dominated web.\n Cue horrific memories of Internet Explorer.\n Of course, Apple\u0026rsquo;s Safari still technically holds its ground and is unlikely to make any moves towards Google since mummy and daddy are in a bit of a standoff over how the web should work, but since Safari is no longer cross-platform its impact is limited, and since it\u0026rsquo;s proprietary it isn\u0026rsquo;t filling Firefox\u0026rsquo;s shoes any time soon. What are we left with then? Vivaldi? Chromium-based. Opera? Chromium-based. Brave? Chromium-based. Microsoft Edge? Chromium-based. Chromium? I don\u0026rsquo;t know about that one. I\u0026rsquo;ll have to check.\nWhy does any of this matter, though? Surely having a shared engine between all browsers is a positive thing? Well, as somebody who develops for the web I can confirm this would indeed be a hell of a lot easier, but it\u0026rsquo;s also a monopolistic nightmare. Blink, the rendering engine used in Chromium and its offshoot browsers is developed in-house by Google. In consortium with other browser engine developers such as the Gecko team at Mozilla and the Webkit team, they are responsible for interpreting and proposing new specifications relating to how the web works. If Mozilla were to go away one day, it would leave Chromium-based browsers even less challenged than they currently are, giving them undisputed control over how the web is intepreted for its users.\nMozilla\u0026rsquo;s role as a non-profit and the producer of the world\u0026rsquo;s second most popular browser engine is a very important one. Its mission statement is to protect the privacy and rights of users online, and it uses its position of relative strength to fight for these rights in the decision-making process. Without it, we will basically have Apple and Google shouting at one another about how to align a div. Fucking nightmare.\nThese are all bigger-picture problems, though. Let\u0026rsquo;s think about the other ramification of Firefox disappearing: we will be down the most popular free and open source browser available for free desktop users. All browsers based on Chromium – even the open source ones – have the issue of being based on a browser which has unclear licensing according to the FSF and code that includes calls home to el goog. While many browser projects attempt to remove these, it\u0026rsquo;s a herculian effort and ultimately you\u0026rsquo;re still left with a homogeny problem in the browser space.\nBut wait a second. There\u0026rsquo;s a browser that\u0026rsquo;s been sat under our gnoses for quite a while now. While some people may not gnow it, we\u0026rsquo;ve had a perfectly suitable, free software browser to call gnome (I\u0026rsquo;ll stop this immediately). That\u0026rsquo;s right: the venerable GNOME Web.\n Crickets\n Fine, I guess I\u0026rsquo;ll explain.\nUnless you\u0026rsquo;re a user of Elementary OS or Bodhi Linux, or are just the world\u0026rsquo;s biggest GNOME fan, you\u0026rsquo;ve probably not encountered GNOME Web before. Also known by its codename Epiphany, GNOME Web is a simple, GTK-based web browser based on WebkitGTK (an implementation of Webkit, which was originally developed as a collaboration between Apple and the KHTML team and also powers Safari). It\u0026rsquo;s a nice little browser which has good integration with GTK-based interfaces such as GNOME, Pantheon, and XFCE (and has decent integration with KDE because KDE and GNOME have worked pretty well together for quite a while now). It may lack a few of the nicer features that come with mature browsers like Chrome and Firefox, but it does the essentials pretty well:\n Website installation: not to be confused with PWAs (which only Chromium properly supports), GNOME Web allows you to install sites to your computer as sandboxed apps Ad blocking: this is built in to the application itself, although it\u0026rsquo;s not enabled by default (I think it is in the latest versions on Flathub). It doesn\u0026rsquo;t work as well as uBlock Origin. For example, it sometimes fails to block in-video ads on YouTube, but you should really be using youtube-dl at this point anyway because YouTube is just awful General website rendering and support is pretty good. Ironically, elementary\u0026rsquo;s website doesn\u0026rsquo;t work in the latest version. It just crashes. Whoops. But apart from that I\u0026rsquo;ve had no issues  The biggest ommission I\u0026rsquo;ve come across is the lack of extension support, which apparently they would like to support through the WebExtensions API. This is a pretty glaring hole for some people, I\u0026rsquo;m sure, but this brings me to my first point:\nWhy should I bother?  For the good of the software: Software without users is just a bunch of code. If there is genuinely no desire for it to exist, it will soon cease to exist. There is no point in an individual or group of individuals pouring effort and time into maintaing a project if nobody uses it. If more people use GNOME Web, it will demonstrate a need for it, introduce more developers and testers to the project, and also (maybe) convince some web devs to clean up their designs For a more consistent experience: Particularly if you\u0026rsquo;re a GTK user, you might be surprised at how nice it is to use a browser that\u0026rsquo;s actually integrated into your system. Firefox does okay with GTK, Chromium looks like utter shit, and GNOME Web is completely integrated, exactly as nature intended Because it\u0026rsquo;s in our hands: GNOME Web, much like GNOME itself, is a project that is entirely in the hands of the free software community. We get to decide how it\u0026rsquo;s shaped and what direction it takes Because it will soon be in our pockets: With the introduction of phosh and phoc, GNOME is coming to the Librem 5 and PinePhone, and with it we will be getting GNOME Web (by default on the Librem I presume). More users testing and prodding this could make it a really nice smartphone experience, since you\u0026rsquo;ve all been complaining for years that \u0026ldquo;GNOME looks like a tablet\u0026rdquo;  Okay. But Does It Work? Yes. For the most part, GNOME Web functions absolutely fine. It plays video, it plays music, it has a nice reader mode. What won\u0026rsquo;t it do? Some important stuff like video conferencing on Jitsi and all of that stuff we probably shouldn\u0026rsquo;t be doing anyway.\nOne thing that GNOME Web does not have is support for DRM. That means no Spotify, no Netflix, no… Hulu? I don\u0026rsquo;t know. I\u0026rsquo;m out of touch on all of this. Some people may see this as a bad thing, but I can categorically say this is actually the correct way to work with the web. Web content should not contain DRM in the first place, and browsers that support it are actively harming users by supporting it.\nDo I believe that these services should be inaccessible? No. But I do believe the default browser on an operating system like GNU/Linux should be as free as possible, and that if users really really want to watch Netflix/listen to Spotify on their machine it should be a conscious decision to go and download another browser to do so. As mentioned above, most distros of Linux include Firefox/Chromium as their default browser, with shockingly few even coming with GNOME Web installed or other free browsers like Midori. On the one hand, this is great for users coming from Windows/macOS as it means they get the same experience they\u0026rsquo;re already used to. But on the other hand, it means free software is betraying its mission. Do users care? Probably not. But I care, damnit.\nI\u0026rsquo;ve been using GNOME Web for a while now alongside Firefox and have started using it more following this news. It\u0026rsquo;s not perfect, it lacks some things I really liked about Firefox, but I do like the fact that my computer now doesn\u0026rsquo;t make the sound of a poorly maintained Boeing 747 during my daily visit to the hamster dance website.\n","permalink":"https://sporiff.github.io/posts/in-defence-of-gnome-web/","tags":["epiphany","gnome","tech","mozilla","firefox","internet","browser"],"title":"In defence of GNOME Web"},{"categories":["blog"],"contents":"In December 2019, I was given a new role at work. This was with the understanding that there would need to be a transition period while I trained my replacement (I work with a very big, poorly documented product, which means that training is a very big task), but that I could expect to be out of my current role within 6 months.\n7 months later, we have not even appointed somebody to the new role.\nNow, of course the COVID pandemic has had a big role to play in this as the business has had to scramble to work out how it functions as a remote workplace and the hiring market is unsurprisingly much sparser than previously. However, my expectations had been structured and I expected the business to make good on this with a slight delay.\nFollowing a series of interviews we had found some very promising candidates and were looking forward to hiring one in particular, who unfortunately had to turn the role down due to moving away. This was a great shame, but I had been assured that in this circumstance we would hire our second choice so that we could get the ball rolling on the training and my transfer. This didn\u0026rsquo;t happen, however, and we are now back to the drawing board.\nAll the while, I have had more work piled on top of me. I am already closing down 2/3 of the business' total support tickets by myself, as well as being responsible for the design and upkeep of our help centre platform and filling in the role of the business' technical writer (I currently have a project to document our dozens of API endpoints, all without access to the code. Fun.). But now, on top of this, I am being given work to do which relates to my new role including customer calls, project scoping and quoting, and more bespoke technical writing.\nAs a result, I have almost completely shut down over the last month or so. I\u0026rsquo;ve not communicated much with anyone outside of a few interactions within the Funkwhale community. I have been able to achieve very little in terms of my own personal projects, and have made basically no progress in my personal or professional development. I have developed an odd sensation whenever I close my eyes that I am falling, which means I now only get about 3-4 hours of sleep a night due to not being able to fall asleep until I am so exhausted I can no longer function. This is also making it harder to work the next day so I\u0026rsquo;m falling behind.\nAnyhow. This is just me venting really. I\u0026rsquo;m annoyed at myself for letting my work have this great an impact on my life, and for not being able to keep fighting through it with vim and vigour, but that is how it is.\nSome of the small things I have been able to achieve lately:\n I\u0026rsquo;ve implemented a dark theme on this site which should follow your device\u0026rsquo;s colour scheme preference I\u0026rsquo;ve added a Funkwhale feature to my Matrix chatbot which allows you to search for %tracks, %albums, and %artists from within a chatroom by supplying your Funkwhale endpoint. At some point I\u0026rsquo;ll look at implementing Oauth and potentially make it more generically useful so I can split it off as its own Matrix bot I\u0026rsquo;ve fixed an annoying bug with this site where code boxes didn\u0026rsquo;t render properly on smaller devices using Chrome. Every other browser worked fine, just Chromium-based browsers took issue with the pre-wrap: whitespace setting so I had to do some fiddling to find a solution that worked on all browsers. Long story short: stop using Chromium. It\u0026rsquo;s absolute dogshit I\u0026rsquo;ve fixed a couple of minor issues on Funkwhale, namely an issue with the playbar rendering on landscape tablet devices, an issue with artist embedding not working properly, and support for unauthenticated users hitting the logout page directly. Just a few small things, but it feels good to get something done  ","permalink":"https://sporiff.github.io/posts/burnout/","tags":["personal","blog","mental health"],"title":"Burnout"},{"categories":["blog"],"contents":"This website uses coleslaw as a static site generator. Coleslaw is managed using Roswell. Some of my bots are written in lisp and packaged using Roswell. It\u0026rsquo;s safe to say I like Roswell.\nAs mentioned a little while ago I\u0026rsquo;ve been teaching myself how to write FreeBSD ports. Since I run this site on a FreeBSD box, it is only logical that I would try to package the tools I\u0026rsquo;m using. So back in February I managed to get a port for roswell working and submitted it for review. Yesterday I got an email from a port committer asking me to confirm my email address as I\u0026rsquo;d recently set up my own mail server and it was now different to the maintainer address listed in the port. To be honest, I\u0026rsquo;d pretty much forgotten I\u0026rsquo;d even submitted it at this point, and decided to bring everything up-to-date including my email address and the most recent version of the package. By this evening, the port had been committed and I could install roswell direct from the ports tree \\o/\nSo, yes. Long story short, if you\u0026rsquo;re looking to install roswell on FreeBSD this is now possible with a simple:\ncd /usr/ports/devel/roswell sudo make install Or to run it locally (as you should)\ncd /usr/ports/devel/roswell sudo make PREFIX=/usr/home/\u0026lt;username\u0026gt;/.local install Tara!\n","permalink":"https://sporiff.github.io/posts/roswell-for-everyone/","tags":["freebsd","lisp","roswell"],"title":"Roswell for everyone!"},{"categories":["blog"],"contents":"A little while ago I made some significant changes to this website\u0026rsquo;s design, asset delivery, and build system. Recently, however, I decided to do some benchmarking against a friend\u0026rsquo;s site to see how it stacked up and I was surprised to see that my site was still significantly slower than it could be, and it was also failing some Lighthouse tests when run through Chromium. I decided to take the time to fix these recently and have madethe following changes:\n The site is now served over HTTP/2 by default (I don\u0026rsquo;t know how or why I missed this in the first place) The site now uses generic fonts such as monospace and serif rather than loading custom fonts. This has the added benefit of making it better for accessibility and end-user customization The CSS has been put through the ringer to ensure that absolutely no unused rules are included, allowing me to shrink the resulting minified file by about 2.5kb The favicon, previously a multilayered .ico file is now served as a much smaller single layer .png (I don\u0026rsquo;t think there was any reason that I was keeping the multi-layered version around since it\u0026rsquo;s only used in the header bar) The footer CC BY-SA 4.0 banner image has been replaced by a single link  As a result of all these changes, the site should now load twice as fast. Obviously, images still take some time to load but given the changes made in the build process this should not block a page\u0026rsquo;s load (I only rarely use images as an illustrative tool anyway, so they\u0026rsquo;re not really essential).\nOne other important thing I was able to do as part of this most recent sweep of changes is ensure that the site\u0026rsquo;s colour scheme is up-to-code with accessibility rules. Since the scheme was pretty much directly copied from Plan 9 I didn\u0026rsquo;t really doubt that this would be the case, but it\u0026rsquo;s good to make sure. I\u0026rsquo;ve updated the CSS to make all colours variables now, so should I choose to implement different themes in future it should be trivial to implement these.\nMy site still isn\u0026rsquo;t as fast as my friend\u0026rsquo;s (he just has a much smaller stylesheet, there\u0026rsquo;s not much I can do there), but I do score a lot higher on accessibility. I\u0026rsquo;m going to call that a win for today.\n","permalink":"https://sporiff.github.io/posts/more-speed/","tags":["speed","website","design","technology"],"title":"More speed improvements"},{"categories":["blog"],"contents":" Your browser does not support the video tag.  I\u0026rsquo;ve been having some fun learning Python recently as part of a course I\u0026rsquo;m doing. One of our early assignments was to create a simple saponification calculator which would return the amount of lye and water required to make soap from some given oils.\nMy personal goal in all of this is to be able to make graphical programs, as I always seem to fall at that hurdle. I\u0026rsquo;ve made terminal-based scripts and apps in the past, but have always given up with the creation of a GUI due to not understanding how on earth you get a front-end to talk to a back-end.\nAfter spending some time with Glade and the excellent GTK developer documentation (and by time I mean I started this rewrite at 7PM and ended up with something workable at 7AM), I am finally getting a handle on how these things talk to one another and work together.\nSomething I\u0026rsquo;d never really considered was just how different interacting with applications in the terminal and on the front-end is. With a terminal app things are fairly sequential and predictable, but when you start putting things into an interface it becomes a completely different process. The original script essentially just ran through each part in sequence:\n Enter a value for olive oil Enter a value for coconut oil Select whether or not to use lye reduction Calculate results  But with a GUI application, it\u0026rsquo;s possible to have all of the inputs accessible at once, and the expectation of the user will be to have either:\n A button that performs the calculation once everything is filled in A calculation performed whenever a value is changed  For the challenge of it, I opted for the latter, which meant that at every turn entries had to be sanitized and handled synchronously. What I had originally thought would be a simple case of just attaching a front end to a script basically left me with a completely different program with a different project structure and set of tools. The only thing that remained consistent was the maths.\nI\u0026rsquo;ve learned a lot about Python recently, to the point where I\u0026rsquo;m starting to think about work projects in terms of how they could be simplified using the language. The important thing, I think, is to make sure I know ahead of time whether I expect to use a GUI or not, as it has a massive impact on how everything is set up.\nI will say that there is a relatively small amount of documentation available for Python + Glade, and a lot of the documentation that is available is aging badly (Python 2.7, anyone?). However, a lot of the blanks can be filled in by reading documentation for Glade and other languages like C and Vala to get an idea of how the elements interact with the language, then simply convert that to your Python thinking.\nIs my code good? No. It badly needs to be refactored and cleaned up. But for a 12-hour sprint with a tool I\u0026rsquo;ve never used before I\u0026rsquo;m pretty happy with it. Of course, the moment I use it on a different DE the themeing falls apart but oh well…\nNext up: learning how to package this stuff.\nSource code\n","permalink":"https://sporiff.github.io/posts/python-programming/","tags":["python","programming","gtk"],"title":"Python programming"},{"categories":["reviews"],"contents":"Just when I was worried I would never be able to have fun with computers again, along comes Plan 9 (well, technically 9Front, but it\u0026rsquo;s much of a muchness). What\u0026rsquo;s Plan 9? Do you care? It\u0026rsquo;s an operating system and you probably don\u0026rsquo;t, but you should.\nAs you can probably tell by this website\u0026rsquo;s recent theme update I was rather taken with Plan 9. It\u0026rsquo;s a quaint little operating system with a whole bunch of interesting ideas (distributed computing, amazing interoperation between the UI and the system, and the most fun you\u0026rsquo;ll ever have with window management). Upon installing it on my old X200s I was instantly warmed by its lovely colour scheme, odd and strangely intuitive manner of working, and the 9Front developers'… unique sense of humour. The whole thing just feels like a return to adventure with computers that I\u0026rsquo;ve not experienced for a while.\nUnfortunately, with only one ethernet port in my house and an unsupported WiFi card (the one time Intel trumps Atheros!) my experience with the system was shortlived. I had wanted to get myself more acquainted with the plumber, acme, and various other neat tools on the system but without functioning networking I was finding myself a bit too limited. I\u0026rsquo;ll probably try to pick up another cheap ThinkPad with supported hardware at some point so I can have a machine dedicated to learning it. It\u0026rsquo;s got some really cool things going for it.\nThe idea of running different computers with different dedicated tasks (storage, computation, interaction) is something that I really wish could have taken off. If we think about how \u0026ldquo;cloud\u0026rdquo; computing works these days, Plan 9 was well ahead of its time. The difference is that you would be in control of a Plan 9 system, whereas cloud computing exists only to bleed you of data and coin. It\u0026rsquo;s a shame.\nI\u0026rsquo;m particularly taken by the system\u0026rsquo;s default (and easily customized) colour scheme. The pastel colours are designed to be calming and easy to look at for long periods of time. I wonder what would have happened to the world if a system like this had taken off instead of the black on white shit that we got from Microsoft (and that we are still recovering from, with dark theming only just becoming a viable option in recent years). As somebody who struggles with eye strain, I find rio a lovely interface.\nAnyway. On to Haiku next I suppose.\n","permalink":"https://sporiff.github.io/posts/plan-9-fun/","tags":["plan 9","9front","tech"],"title":"Plan 9 fun"},{"categories":["rants"],"contents":"A couple of years ago, I cancelled my Spotify subscription. In fact, I completely discontinued my use of the platform. My reasoning at the time seemed petty, but everything I\u0026rsquo;ve learned and listened to since has convinced me that this was the right way to go.\nBack in those days, I didn\u0026rsquo;t really have a problem with Spotify. I am a musician myself, so I wanted to contribute to my favourite musicians even though I lacked the funds to buy every new album. Spotify seemed to answer this desire with its vast catalogue and seemingly reasonable subscription fee model (as well as its ad-funded \u0026ldquo;free\u0026rdquo; model), so I happily paid month-to-month for access to this service and thought no more of it in the over 1 year I stayed with the service.\nI can\u0026rsquo;t remember exactly which album it was (I want to say They Might Be Giants' The Spine, but I wouldn\u0026rsquo;t swear to it in court), but something up and disappeared one day with no warning. Something to which I had previously had access and something which was previously present in my playlists and radios was just… gone. My guess is that there was some licensing dispute or contract expiry that led to this, but it was that event that made me realise quite clearly that I meant nothing to this company and had no say over its operation, to the point where things would simply be removed without so much as a by-your-leave or an offer to compensate for the loss.\nIn many ways I am thankful for this. After all, it was this that set me free from Spotify\u0026rsquo;s disgusting clammy grasp and set me on my search for a self-hosted alternative – a search which would lead me to Funkwhale, a project with which I work and for which I have a great deal of love. In all honesty, I hadn\u0026rsquo;t really thought about Spotify in a long time until very recently when somebody brought up the subject of podcasts in Funkwhale following the 0.21 release. They mentioned that a certain podcast (The Last Podcast on the Left) had gone Spotify-exclusive, and were expressing disappointment that this would mean they couldn\u0026rsquo;t load it in to a Funkwhale channel.\nI must have known, in the back of my head, that Spotify might do something like this. It\u0026rsquo;s the kind of shitty thing that successful technology companies are wont to do, after all. But I was still surprised that it had actually happened. I went and did some reading up about it on various forums and was horrified by some of the dismayed responses of the show\u0026rsquo;s fans, who expressed disappointment not only at the fact that they didn\u0026rsquo;t like Spotify as a platform, but also that some people would simply be unable to access the content now due to the fact that Spotify does not operate in certain countries.\nPodcasts and the point I will admit I\u0026rsquo;m not the biggest consumer of podcasts. In fact, up until the last few years I\u0026rsquo;ve listened to very few indeed. However, I\u0026rsquo;ve known about them for about as long as they\u0026rsquo;ve been around. Podcasting is an interesting idea: an asynchronous radio broadcast that can be mirrored and spread across the world using a set of standard tools that anybody can use and build from. This kind of open process makes the medium very disruptive and far-reaching; the concept can be applied with very affordable and easily-accessed tools and isn\u0026rsquo;t limited by licensing. It\u0026rsquo;s ideal for underground radio, sensitive ideas, and entertaining conversations of all kinds.\nWhy, then, are we so quick to give this up?\nOn the part of podcasters, I kind of understand it. The process of creating regular shows with research/writing/editing etc. is one that is not without expense. Some podcasts eventually become businesses unto themselves, and these require sponsorship or other means of funding so that the creators can continue to work on the project full-time. In step companies like Spotify, Apple, Google, etc. who can wave fat stacks of cash around knowing full well that they will be boosting their subscriptions massively by bringing these shows on board in an exclusive contract. This all makes sense. What doesn\u0026rsquo;t make sense is that people actually do pay. People are happy to switch to Spotify and either buy a subscription or listen to the ad-supported service, accepting that this is just how it is now. Why? Do we not see that the move made by these creators completely defeats the purpose of the medium?\nI remember writing an article in university about how television – by its original definition – is not and can never be art. It was a contentious and sophomoric piece, rather less well thought out and more forceful than I am proud to admit, but the kernel of the idea still rings true to me. Art is expressive, subversive, and highly individual. Film can be produced out of an individual\u0026rsquo;s pocket and distributed by shining a projection in a home or on the streets, painting can be achieved with a range of materials and shown anywhere, music can be performed live or recorded with very rudimentary equipment etc. etc. Television? No. By definition television must be produced by companies and is inherently stunted by this fact. To get access to the platform of contemporary television, one needs the help of a network of some description, or a public access channel with its own set of rules. You can make videos, of course, but these are not television until and unless they reach a television set. That is the meaning of television and by that meaning it can only be as subversive as it is allowed to be.\nThe point of all of this is that podcasting is not television. By definition it is not restricted by corporate interests or rules created by external parties. Just like other artforms it can be cheaply produced and shared without defying the meaning of the platform. To have a podcast be platform exclusive and inaccessible to the wider public means that it is no longer a podcast: it is corporate radio.\nThe technology industry The technology industry as a whole is despicable and disgusting. Allying engineering and software development with marketing and a corporate agenda is an inherently dystopian idea. Technologists on the whole are generally just interested in the creation of new things and making things work in interesting new ways. But just as a scientist may see only fascination and intrigue in the study of subatomic particles and radiation, a military will see a weapon. So too is it with the technology industry. Knowing of humanity\u0026rsquo;s propensity for laziness and the desire to make our terrible lives slightly easier, companies coax technologists into creating modern marvels and then wield them as weapons of enslavement to bleed an unsuspecting public of their life blood while simultaneously patting themselves on the back and wrapping an arm around you as they whisper \u0026ldquo;see? We LOVE you\u0026rdquo; through a serpentine smile.\nThe technology industry is not, of course, alone in all of this. The difference between it and other industries is that the correct way of doing things, the moral and correct way, exists and has been a viable alternative for years. The free software movement exists for the express purpose of providing people with a way to use their technology without getting proverbially fisted by companies and used as a ghoulish, grinning puppet spewing dollar bills from its mouth. But marketing is a powerful thing and is not to be underestimated. At every turn, free software is dismissed as not shiny enough, too hippyish, too geeky. The shiny shiny is what I desire. Only later do people realise how bad things are getting, but by then the walls are closing in behind them and the exit is too small to see.\nWhen I worked in the education sector I was always dismayed by the UK\u0026rsquo;s acceptance of proprietary software as tools of teaching. Students would learn to create in Adobe Creative Cloud (which, as a former editor, I can assure you is just about the worst fucking tool you could ever have the displeasure of using) and would then be left in the cold once they left if they could no longer afford to pay the monthly fee to access it. Similarly, tools like Office365 are imprinted upon people from an early age as a necessity, something which will always be needed. Of course, if you can\u0026rsquo;t afford it you\u0026rsquo;re shit out of luck and frankly not good enough as a human being to join the normal world that the British education system paints for you. Once again, control of education and creativity being handed to companies who care about nought but the coin is an inherently evil idea, and the way these companies lock you in by design is an inherently evil act.\nArt and technology are natural bedfellows, but art and the technology industry is a match made in a level of hell Dante was too afeared to even dream of devising. Handing art over to powerful companies with not only the hand rubbing greed of a bottomless pit wrapped in the body of a pig but also an army of intelligent and subservient – or worse, willing – technologists is about the worst thing we as a species can do. I know that people despise slippery slopes, but they are a very real thing and we are already slipping down this one. Just as television requires you cowtow yourself to a company or collective, so too does releasing art on walled garden platforms such as Apple, Spotify, Google, Netflix. By putting your creations on these platforms, you are announcing only that they have been deemed okay by those companies.\nI understand that there is a difficult balance to be struck here. Artists need to earn money to live, as do technologists. But an artist who ceases to make art is no longer an artist, and a technologist who works making proprietary software to further nothing but a corporate agenda is always making the wrong decision and is culpable for the actions of its employer. As Drew DeVault recently said in a blog post:\n Mass surveillance, contempt of the law, tax evasion, oppression of the poor, of minorities… this is what our industry is known for, and it’s our fault\u0026hellip; But, maybe you would object, maybe you would have the courage to say “no” when asked to do these things. Maybe you would, but someday, a cool project will come across your inbox – machine learning! Big data! Cloud scale! It’s everything you were promised when you took the job, and have more fun with it for a few months than you have had in a long time. Your superiors are thrilled – “it’s perfect!”, they say, and it’s not until they take it and start feeding it real-world data that you realize exactly what you have built. Doublethink quickly steps in to protect your ego from the cognitive dissonance, and you take another little step towards becoming the person you once swore never to be.\n To summarise This has been a long and rambling rant, and I don\u0026rsquo;t promise it makes any real sense (I\u0026rsquo;m not really going to bother going back to edit/proofread it too much). In the course of the post the point changes frequently, but the gist is something that plays on my mind a lot as a creator and a technologist.\nTo Spotify and its employees: what you do is evil, and it will always be evil no matter how you try to paint it. You exist solely to take control from creators, money from audiences, and dignity from yourselves. The same goes for the rest of the big companies daring to get involved with art: get your filthy tentacles away from us.\nTo artists: please look in to free software solutions. They exist, they are very good, and best of all the technology is not created to the specifications of a literal cave goblin.\n","permalink":"https://sporiff.github.io/posts/walled-gardens/","tags":["tech","technology","spotify","funkwhale","walled gardens","rant","art"],"title":"Ditch the walled gardens"},{"categories":["blog"],"contents":" I got poking around at this site and was doing some thinking. Mainly along the lines of speed and security. By default, the dark-hyde theme provided by the good folks over at Coleslaw uses Google\u0026rsquo;s CDN for font delivery. This is all well and good, however I\u0026rsquo;m really not a big fan of Google at all and like to avoid them where possible. With that in mind, I started thinking about how I could go about replacing them.\nThe major thing that Google provides on a lot of websites, even those who are privacy-conscious enough not to use abominations like Google analytics, is the serving of fonts and Javascript. Hosting these things through their CDN makes it very straightforward to plug a font or script in without having to worry about hosting the files locally or anything like that. However, it also means that people\u0026rsquo;s browsers are constantly calling out to Google, thereby increasing the web\u0026rsquo;s reliance on them.\nTo that I say: eff off.\nReplacing these fonts was reasonably straightforward thanks to the magical combination of Digital Ocean\u0026rsquo;s CDN and the google-webfonts-helper, which made grabbing the relative files and CSS a breeze. Pointing all parts of my theme file to the newly cached CSS files and migrating all images etc. should speed things up at least a little bit, and best of all: no Google! All fonts should have a backup in case DO is a little slow. It\u0026rsquo;s been known to happen.\nThe other little hacky thing I did was relates to my earlier post about using a git-based workflow to manage the deployment of this site. I\u0026rsquo;m a fan of the idea of using lazy loading for images since they\u0026rsquo;re not particularly important for this site\u0026rsquo;s content and I\u0026rsquo;d rather let them just load in their own time. However, Coleslaw only produces raw images from Markdown, with no option to add the loading tag. Oh well. This site runs on FreeBSD, baby! Anything is possible. A simple one-liner added to the deployment hook should allow me to stick that in to the files.\nfind . -type f -name \u0026#34;*.html\u0026#34; -print0 | xargs -0 sed -i \u0026#39;\u0026#39; -e \u0026#39;s/\u0026lt;img /\u0026lt;img loading=\u0026#34;lazy\u0026#34; /g\u0026#39; Oh, it\u0026rsquo;s hacky. But it works. I\u0026rsquo;m not really in the mood to go diving through Lisp code to do this in Coleslaw…\n","permalink":"https://sporiff.github.io/posts/site-speed-improvements/","tags":["tech","blog","hosting","technology","website"],"title":"Site speed improvements"},{"categories":["reviews"],"contents":" For a good long while now I\u0026rsquo;ve been hosting a bunch of different services across different servers. Given that I started this tradition before I was particularly proficient with GNU/Linux and *BSD, I have usually taken to using Ubuntu as a crutch of sorts. Now, there\u0026rsquo;s nothing wrong with Ubuntu per se, but it nearly always has more than I need preinstalled and removing software is a bigger risk than adding it.\nI\u0026rsquo;ve used Alpine before, believe it or not. I actually use it all the time in Docker containers, so I\u0026rsquo;m pretty used to some bits and pieces like apk, the package manager, and OpenRC, the init system. I\u0026rsquo;ve made use of the latter in Gentoo and Artix so I\u0026rsquo;m fairly au fait with it. However, I\u0026rsquo;ve never used something as unusual as Alpine on a desktop. Its use of musl in place of GCC and its focus on biting simplicity by default make it somewhat unsuited (I believed) to powering a home machine.\nIn the first instance, anyhow, I focused on migrating my existing Ubuntu servers over to Alpine. Having recently closed down my Mastodon and Gitea servers to new users, these were the first to go over. All went pretty smoothly, though to be fair the requirement were minimal given that each service is hosted in containers so it\u0026rsquo;s really just a case of using rclone to copy information from one to the other then… recreating the containers. The Funkwhale migration required a small amount of additional warning, but all was done in under an hour. The delay was mostly in DNS propogation for letsencrypt to work.\nAll in all, I\u0026rsquo;ve been able to reduce my operating costs by a not-unimpressive $20 a month by moving these services from one big Ubuntu box to three smaller Alpine machines. The OS is incredibly quick and needs very little love to keep it going, and everything appears to run a lot better (Docker builds are noticeably quicker across the board). This also means it is easier for me going forward to make changes to these services such as scaling up Funkwhale while leaving other services small and intact. It\u0026rsquo;s neater this way and I much prefer it.\nThe desktop Ah yes, I hear you say, but what about the desktop? Well, this was a silly fancy of mine. A system that ran so well on my servers was enticing, to say the least, and the research revealed that — due to the presence of Flatpak in the Alpine repos as well as the large volume of native packages already present — I wouldn\u0026rsquo;t miss out on too much migrating from Fedora to Alpine. Nvidia would probably never work properly due to its dependence on GCC, but I quite frankly never play games. My next machine will not have an Nvidia card, I don\u0026rsquo;t think. I\u0026rsquo;ll shop around for something better suited to Linux.\nThe Alpine wiki doesn\u0026rsquo;t have a patch on the Arch wiki, for sure, although a lot of the information is transferable between the two. Alpine misses some pretty important steps in its setup instructions and expects a great deal of knowledge on the user\u0026rsquo;s part around essentials such as daemon management. My first install attempt was disastrous, not least because I chickened out of a KDE install and reverted to a GNOME install, leaving me with a completely broken GNOME installation that would only work on X11 (weirdly the KDE one only worked properly on Wayland). Having learned my lesson somewhat about planning things ahead of time I mustered up the courage to try again and this time I got it right.\nEven with the full suite of software installed by the gnome and gnome-apps metapackages the system still makes no assumptions for you. This is one of Alpine\u0026rsquo;s major appeals, so I\u0026rsquo;m not knocking it, but as a normal desktop user you just have to be careful about certain packages not being installed as dependencies because they\u0026rsquo;re not explicitly required. This can mean a lot of scratching your head over why the thing you installed isn\u0026rsquo;t working: installing that package works fine in Debian!\nEven running GNOME (beast that it is) the system is noticeably snappier than Fedora. If I were to start again I think I would ditch the gnome-apps metapackage and plump for installing the system piecemeal with more care over exactly what was needed, but for the time being I\u0026rsquo;m perfectly happy with it. Not a particularly difficult install, smooth running, decent community support. I can see it getting a lot more popular.\nA farewell As previously mentioned Baku Social and Gitsune have been put out to pasture. Both were fun experiments, but ultimately they were more effort/money to manage than they were worth. Both received a high volume of spam accounts that had to be managed and removed, and ultimately the actual interest in them was non-existent. I\u0026rsquo;m going to continue running Baku Social as a personal instance. I like having my own server and I think it\u0026rsquo;s nice to keep a unique handle. Gitsune will be going offline for good, however. I\u0026rsquo;ve migrated everything off of there to Sourcehut, so I will be putting the hosting money to a service that\u0026rsquo;s got a lot more promise.\nAll is not doom and gloom, however. Tanuki Tunes continues to increase in popularity as a testbed for new Funkwhale features. Stepping away from the sheer number of services under my control will allow me to focus more resources and time on Tanuki Tunes should things need to be scaled up at all. Given how well it\u0026rsquo;s running even at the current load this hopefully should not need to happen for a while, but it leaves us with options.\nUntil next time!\n","permalink":"https://sporiff.github.io/posts/alpine/","tags":["alpine","linux","tech","technology"],"title":"Alpine"},{"categories":["blog"],"contents":" I\u0026rsquo;ve been meaning to improve the way I deploy this website for a little while now. Since I stopped using Netlify in favour of a more bare-bones approach, there has been a lot of manual/ugly work involved with the process of updating this website and posting from its RSS feed to my Mastodon feed.\nInitially, I simply cloned the git repository for this site to my FreeBSD box and then SSHd in to the box to pull the lates files. This, of course, is not very automatic, and it also required me hosting a lot more files in the git repository than strictly necessary since I was performing the coleslaw build on my local machine and pushing this up to replace the directory. This was always highly inefficient, but as usual I kind of just though \u0026ldquo;eh, I\u0026rsquo;ll get round to doing something better later\u0026rdquo;.\nThen we have my bot. I was so pleased with this little thing because it was my first endeavour at writing something in common lisp. However, it too was pretty dumb. In both cases I eventually just ended up setting up cron jobs: one to perform a git pull every 15 minutes and another to run the scbl --load for the bot 5 minutes later. Yuck. Not exactly a dynamic way to do things.\nSince I had a bit of time on my hands today and have also been playing around with sourcehut, I figured this was a good a time as any to improve some things. I had two objectives:\n Run the git pull only when a new commit had been pushed Immediately run the RSS scraping bot after the repository had been updated so there was no delay  I knew the former of these was going to be best achieved with a git hook, but since I\u0026rsquo;d never actually used one of these before I really had no idea what I was doing. Luckily, smarter people than myself have already done this and have been kind enough to write it down.\ntl;dr:\n The server now no longer hosts a working git repository, but instead just hosts a deployment A post receive hook picks up the changes and builds the pages on the server (no more pushing a full folder of web pages, hurray!)  Next, we have my bot. This thing is a pretty simple common lisp web scraper that functions pretty much identically to my folklore thursday bot. As I said earlier, this bot was basically just powered by a cron job that tried to sbcl --load the file every 15 minutes or so. This, of course is pretty wasteful for a couple of reasons. Firstly, it\u0026rsquo;s just bad practice to run the thing at intervals when I may not be posting for… months. Secondly, running the REPL rather than just using a binary is pretty expensive. Roswell to the rescue!\nRoswell is a Lisp implementation manager and one of my favourite things ever. In addition to making managing your lisp implementations a breeze, it also allows you to package your lisp scripts pretty trivially. With a few minor changes, I could simply install the bot as a binary and add it to the path of the git_daemon user. Bingo. Now, the post receive hook runs the bot after receiving a new post.\nThis has been a fun little exercise. I should probably have got round to this sooner seeing as I\u0026rsquo;ve been in lockdown for a few weeks but unfortunately I\u0026rsquo;m too lazy to be lazy.\n","permalink":"https://sporiff.github.io/posts/building-smarter/","tags":["git","website","tech","bot","lisp"],"title":"Building smarter"},{"categories":["blog"],"contents":" As mentioned in my previous post, I\u0026rsquo;ve recently been playing around with FreeBSD on one of my laptops. Why? Boredom, frankly. Boredom with my job, boredom with my lack of social life, boredom with the weather. Whatever the cause, the cure is always computer fun!\nIf you\u0026rsquo;ve ever used FreeBSD you\u0026rsquo;re probably already well aware of the ports system. It\u0026rsquo;s the thing that scares people because you mentioned the dreaded \u0026ldquo;S\u0026rdquo; word\n Oh, yeah. I usually build that from source to be honest\nYou WHAT\n I\u0026rsquo;ll admit to being more of a binary package man myself (my time is… well not valuable. But still ticking away!) and I only very rarely dip into the ports tree when a binary doesn\u0026rsquo;t exist or is a long way out of date. But there is a lot of comfort having it there as a backup.\nOne piece of software I use quite frequently is Roswell, a lisp implementation/installation manager. It\u0026rsquo;s what I use to install and manage coleslaw (you know, that thing that powers this website). Now, being a spoiled boy from the good side of town over with that Void and Arch bunch, I\u0026rsquo;m used to having Roswell nicely packaged up for me to install at the tippety tap of my fingers. On FreeBSD, this package is unfortunately missing. Ho hum. No worries, thinks I, I\u0026rsquo;ll just install it from source and move on with my life.\nBut I got to thinking: how hard is it to actually make a port for something? Looking through the rather excellently written Porter\u0026rsquo;s handbook it looked simple enough, especially since Roswell is essentially a good old fashioned ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install jobby. So I started poking around at it and seeing if I could make sense of the syntax and the logic behind it.\nAfter a few hours of plugging, it looked like I had something usable. It took a bit of swearing and messing up my Makefile (don\u0026rsquo;t believe everything portlint tells you, he\u0026rsquo;s a fibber that one), but eventually I could get it successfully installing, deinstalling, configuring with a PREFIX, and, well, behaving like a package should.\nI\u0026rsquo;ve had a crack at packaging before with .deb and guix. It\u0026rsquo;s always come across as the single most maddening thing in the world to me, so pernickety and demanding for something that should be so simple. Writing the port, on the other hand, was relatively straightforward. You tell it what is required, pass it some special instructions, and just let the standard tools take over from there. This form factor, coupled with the quality of the documentation made this a much less frustrating (and much more rewarding) packaging experience. Granted, I picked a very simple thing to start with. These things get harder logarithmically.\nAnyhow, since it worked for me I decided to actually submit the thing upstream and see if it\u0026rsquo;s good enough to be included.\nOnly time will tell.\n  ","permalink":"https://sporiff.github.io/posts/playing-with-ports/","tags":["FreeBSD","unix","floss","free software","ports","lisp","roswell"],"title":"Playing with ports"},{"categories":["reviews"],"contents":" As mentioned in an earlier post, I\u0026rsquo;ve recently started hosting this site on a FreeBSD box to help me learn FreeBSD a little bit better and become a bit more bilingual with my UNIX-like systems. I had initially planned to install FreeBSD on my X200s, but unfortunately the OS was not supported by Libreboot, so I tabled the idea for a while.\nThe issue with this, of course, was that by limiting my use of FreeBSD to one very small server which had very little to do I would not really get to experience the benefits, drawbacks, and differences the system offered when compared with GNU and non-GNU Linux. No, if I wanted to get a real taste of the system, I had to get it installed on something that I could actually use day-to-day.\nThe dilemma This X200s laptop was purchased specifically for its ability to run Libreboot, a libre software replacement for proprietary BIOS/UEFI implementations. While Libreboot has served me very well for the time that I\u0026rsquo;ve owned this laptop, the project seems to have slowed to a crawl. There has been no release in nearly 4 years, and the new project to which it was due to rebase has seen no development in over 3 years, so the likelihood of seeing a new release feels somewhat remote. Work is certainly still being done, but the pace is glacial and certain issues are unlikely to ever be resolved.\nLibreboot currently uses another project as its upstream: Coreboot. Similar to how Linux-Libre operates, Libreboot strips any proprietary remnants out of Coreboot in order to ensure that your boot firmware is as free as possible. However, this means that:\n The supported hardware list is very short The supported software list is somewhat limited  As mentioned, I don\u0026rsquo;t really have to worry too much about point #1 as I have one of the pieces of equipment that is supported fully by the project. However, after several attempts to get FreeBSD running after reflashing Libreboot back to text mode, it became apparent that it simply was not supported.\nThe solution Luckily, it seems I was not the only person who\u0026rsquo;d sought to scratch this particular itch, and the solution came in the form of a rather excellent blog post by Staf Wagemakers detailing the process of switching from Libreboot and GRUB to Coreboot and SeaBIOS, which fully supports FreeBSD.\nSo, after a slightly nervewracking evening of working through the post and tuning the different instructions to my hardware, I managed to replace Libreboot with Coreboot and get SeaBIOS up-and-running. After some fiddling about, I had the BIOS recognising my USB stick and was ready to boot.\nThe easy bit The FreeBSD installation process is gloriously simple, with a few exceptions. Most things can easily be gleaned directly from the instructions presented on screen at install time, although some other parts (such as WiFi selection) take a bit of reading to grok. Once you\u0026rsquo;ve got it, though, it makes complete sense and the system installation process is a breeze. The chrooting in to the system at the end was also a nice touch (similar to what you see in something like Anarchy Linux).\nOf course, the most arduous bit of any *BSD/non-automated Linux installation is the set up and tweaking of Xorg, which gave me the usual troubles as it shouted at me that my monitors didn\u0026rsquo;t match what it was looking for. After some reading of the FreeBSD Xorg manual I managed to get my config file generated and was able to boot into a minimal X system. Huzzah!\nMaintaining consistency I tend to use i3-gaps, and more specifically a modified version of LARBS (I know, I know, it\u0026rsquo;s a meme). Now, this rice has been specifically tailored to Arch/Void Linux, but with a little bit of tweaking it is relatively trivial to replace everything with a FreeBSD equivalent. Some of the status bar blocks will not work by default as they are looking for pointedly Linux features. Some of these have immediate replacements (such as the battery indicator, a helpful example of which is installed alongside i3blocks), but some others don\u0026rsquo;t really. The most annoying one is the volume indicator, which can easily be controlled by switching out your pulse/alsa controls for something like:\nexec --no-startup-id mixer -S vol +5% \u0026amp;\u0026amp; pkill -RTMIN+10 but this does not update the status. For some reason, the pkill command I\u0026rsquo;ve set up does not automatically update the status. For now, I\u0026rsquo;ve simply set a more frequent update but I\u0026rsquo;ll have a go at working out why it\u0026rsquo;s not working soon.\nApart from that, I\u0026rsquo;ve been very impressed. The ports system is incredibly comfortable to use, as is installing binary packages using pkg. With few exceptions everything I use on a day-to-day basis is available and needs little modification to get working how I\u0026rsquo;m used to it.\nNow all I have to do is get resume on lid open working…\n","permalink":"https://sporiff.github.io/posts/learning-freebsd/","tags":["freebsd","bsd","unix","floss","free","software"],"title":"Learning FreeBSD"},{"categories":["blog"],"contents":"I\u0026rsquo;m always surprised when I see modern companies not using renewable energy sources as a matter of course. It seems so strange to me that they don\u0026rsquo;t see that this is a really big draw, especially in the tech industry where at least 70% of us are weird hippies. The other day I decided to do a little research on my host\u0026rsquo;s (DigitalOcean) offerings and the differences between the datacentre, and I was surprised to find out the following:\n DO does not have an easily accessible page about the green status of their datacentres Only one of their datacentres is easily verifiable as green (FRA1)  Now, judging by this post the majority of DO\u0026rsquo;s European hosting is fully green (including London, though for privacy reasons I would wholeheartedly and emphatically disuade you from hosting anything at all in the UK if you can possibly avoid it). But trawling through the forums shows a lot of umming and ahing on DO\u0026rsquo;s part and a seeming unwillingness to be transparent and straightforward with users about the state of their energy sources.\nOf course, users can go to the hosting providers directly to find out more information, but it really would be better for DO to be up-front and honest about these things from the get-go. If, for example, somebody chose to host their services/data in the SF datacentres, they may be disappointed to learn that these run on only 30% renewable energy and may look to use a different region. Certainly, this would have swayed my decision when setting up my servers.\nCurrently, my Matrix server and website are both hosted in FRA1, as are all of my spaces. I\u0026rsquo;m thinking of migrating my other services (Funkwhale and Mastodon) from AMS3 to FRA1 as well, if only to reward interxion for being so up-front and reliable with their renewables.\ntl;dr, DO sort your shit out and put green ratings on your regions at selection.\n","permalink":"https://sporiff.github.io/posts/going-green/","tags":["renewable energy","electricity","hosting"],"title":"Going green"},{"categories":["blog"],"contents":" I\u0026rsquo;ve been making some changes to my current website setup. Before, this site was hosted on the same server that currently hosts my Mastodon instance and Funkwhale pod, as well as my git server. But with gitea\u0026rsquo;s poor performance and the general overcrowding, mixed in with my recent winding down of my Streama server, I decided to make a completely new server for my website.\nI\u0026rsquo;ve been doing a lot of playing around with FreeBSD lately as part of my never-ending quest to find the ideal operating system. There are some things I absolutely love about the system, and its minimalist setup and hardy security defaults made it the ideal candidate to set my website up on.\nThe initial install was incredibly easy, although I made some mistakes initially with the installation of my SSH key. DO\u0026rsquo;s FreeBSD offering is considerably more hardened than their Debian/Ubuntu ones as they require an SSH key be set up and do not allow the management of root passwords. This suits me just fine as I never use a root password if I can possibly avoid it. A fully featured wheel account is good enough for me.\nAfter getting everything set up nice, my next adventure came with Nginx. I\u0026rsquo;m so used now to setting up sites and web apps using nginx on Debian-based systems that I can do it in my sleep, but the difference in layout/defaults for Nginx on FreeBSD gave me pause for thought. Once I got into the swing of it, it made a lot of sense and felt a lot more suited to smaller setups, but the initial confusion was somewhat annoying. Not the system\u0026rsquo;s fault in any way (RTFM and all that), but still a puzzler.\nThen I decided to be an idiot. See, DO supplies a default FreeBSD account which is installed with your keys. This account has full root access using sudo and is meant to be used for access and maintenance of the server. But I\u0026rsquo;m an odd fellow, so I really wanted to set my account up to better match my other machines. I installed zsh and decided to chsh my account to change my username and shell. All went well, I moved my home folder and then left the shell to load my new shell. Everything loaded fine, but suddenly my site stopped working. Ah, I realised too late that of course the site is just a symlink from a folder in my home folder, which has now moved. Whoops. I\u0026rsquo;ll just move that b-\nsporiff is not in the sudoers file If you change your username, make sure you also change your sudoers file. Fool.\nAnyway, it was easy enough to get everything torn down and built back up. I\u0026rsquo;ve now got the server all up-to-date and automated and I must say I\u0026rsquo;m pretty happy with its performance so far. It hasn\u0026rsquo;t got a big job, but it\u0026rsquo;s a job I\u0026rsquo;m glad I chose FreeBSD to do.\n","permalink":"https://sporiff.github.io/posts/moving-to-freebsd/","tags":["freebsd","unix","bsd","floss"],"title":"Moving to FreeBSD"},{"categories":["blog"],"contents":" 2019 was a bit of a rocky year, admittedly. With the country in which I reside resigning itself utterly to lunacy, my relationship ending, and a general descent in my mental health at several points due to work and existing issues, it is not a year I will necessarily look back on with much fondness. All is not, however, doom and gloom.\nFunky friends Back in February of 2019 I started committing to Funkwhale. Initially I contributed only documentation, but as I spent more time with the application I started to get more involved with the codebase. I now regularly contribute small fixes and improvements, and I even sit on the association\u0026rsquo;s steering committee.\nParticipating in Funkwhale has been an incredibly rewarding experience which I hope to keep going for as long as possible. In February, a whole year on from the start of my engagement with the project, I will be travelling to meet the collected association members in person!\nHosting hijinks Of course, 2019 was also the year that I got fully stuck into hosting my own services. While Tanuki Tunes and Baku Social were technically spun up in December 2018, they didn\u0026rsquo;t really start to see any traction until much later. I now host 4 public services (Mastodon, Funkwhale, Gitea, and Matrix) and a private Streama instance for me and my dad. I had always been interested in self hosting, and I have to admit that I very much enjoy the challenge it presents. Tanuki Tunes has been particularly popular, gaining over 200 users over the course of the year 🎉\nHopes for the future I was recently given a new job, which will mean that I will be transitioning out of my current reactive support role and into a more constructive technical account management role. While not the development role I was hoping to get (in order to help me improve my coding knowledge), this role should leave me with more time to work on projects, better pay, and a chance to actually work a standard shift for once. No more tickets! No more tickets!\nThat is, of course, if the UK doesn\u0026rsquo;t decide to completely tear itself apart over the next year…\n","permalink":"https://sporiff.github.io/posts/a-new-year/","tags":["new year","update"],"title":"A new year"},{"categories":["blog"],"contents":" This marks my third(?) migration to a new site format! 🎉\nI\u0026rsquo;ve recently started learning to use Emacs after leaving the Emacs manual lying around for the better part of a year. Since I\u0026rsquo;ve decided to try my hand at some common lisp I figured it would be a good time to start learning, and my goodness I wish I\u0026rsquo;d started this sooner.\nAs part of this new experience I started looking into ways I could use Lisp in my everyday life. I quickly came across Coleslaw as a recommended static site generator and decided to give it a go. There\u0026rsquo;s a bit of a learning curve to it, and it doesn\u0026rsquo;t really have the simplicity and wider appeal of alternatives such as Hugo and Pelican, but for the sort of thing I\u0026rsquo;m doing it works wonderfully. The config is a joy to write (reminds me a lot of configuring my Guix system settings) and everything can be easily chopped and changed without too much worry. I can highly recommend it.\nAfter years of Vim and VSCodium usage, I\u0026rsquo;ve finally taken the plunge into Emacs and… I completely get it. This program is nothing short of magical. Again, lots of configuration and learning required (I have a 500 page manual sat on my desk for easy reference!) but once you\u0026rsquo;ve got even the most basic commands down and some essential extensions installed you never want to leave it. It\u0026rsquo;s just so damn fun to learn. I\u0026rsquo;m going to see how it goes as my everyday IDE. It\u0026rsquo;s already got me to uninstall Riot, Telegram, and Thunderbird from my machine…\n","permalink":"https://sporiff.github.io/posts/coleslaw-migration/","tags":["blog","website","lisp","emacs"],"title":"Coleslaw migration"},{"categories":["blog"],"contents":" So I\u0026rsquo;ve been having some fun recently trying to teach myself Rust. I\u0026rsquo;m not really a programmer by any stretch of the imagination (yet!) but was challenged to write a bot for a Matrix chatroom I frequent. Most of us have written bots already, so it was my time to try something.\nOriginally I had planned to use Python like I had done previously for my Mastodon bot, tsukumogami. However, I\u0026rsquo;d been hearing some interesting things about Rust and decided it would be a fun challenge to try and write something functional in the language.\nFeatures Weeabot\u0026rsquo;s current features include the following:\n The ability to convert romaji to kana The ability to convert kana to romaji The ability to translate a string to a given language using Yandex translate Various silly responses to triggers  Of these the Yandex call was definitely the hardest to get right. I had originally hoped to use a crate for this, but the one I found unfortunately didn\u0026rsquo;t compile due to depending on old crates which had an old OpenSSL version. After spending some time trying to write the call myself I realised it was easier to fix the crate and use it anyway. I\u0026rsquo;ve since presented this fix to the original maintainer, so hopefully it will be usable again soon.\nSome observations Rust is a challenging language for a newcomer as it is very strict about getting things right before compilation. This is the right thing to do, and thankfully the compiler is excellent at giving instruction on what is wrong and possible ways to fix it. But unlike Python (which really is a pick-up-and-play language in many ways), Rust really does not want you to run something that is broken.\nRust is also probably the wrong language for this project. While there is a nice ready-made matrix_bot_api crate, it is clear that the language is designed for more complex stuff than this. I would probably have had an easier time using a more web-oriented language. Originally the challenge had been to use Dlang, but I was much more interested in Rust.\nSome fun stuff So far I\u0026rsquo;ve learned a lot not just about Rust but also about Matrix, APIs, and deployment pipelines. As a joke, this bot (which essentially just posts shitposts that pastiche typical weeaboo-esque responses) has been set up with a fully functional test environment and pipeline for test, build, and deployment to a live server. This was an interesting challenge as I have previously understood bits and pieces about the process from my interactions with Netlify and Funkwhale, but have never set it up for myself before.\nAnother fun thing has been getting to play with Crates. Crates are amazingly helpful in giving you a good starting point to build from and feel much more convenient to me than Python libs. Most of this bot is just different crates stuck together, although I have actually pushed a fix for the yandex translation crate I used. So I\u0026rsquo;m not just a leech!\nAnyway, the source code for the bot can be found on my GitHub. There are still some features I want to get working, but I\u0026rsquo;m pretty happy with its performance thus far as a translation bot alone.\n","permalink":"https://sporiff.github.io/posts/weeabot/","tags":["tech","chat","communication","rust"],"title":"weeabot"},{"categories":["rants"],"contents":" My workplace uses Slack. Actually, let me rephrase that: some of my coworkers use Slack. The company for which I work has had some trouble committing properly to a communications platform, which has left us in something of a state of limbo when it comes to communicating with one another. Some people only use Slack, others only Skype, still more refuse to communicate by anything other than good old fashioned face-to-face or email.\nNow, don\u0026rsquo;t get me wrong, I like instant messaging. Or more specifically I don\u0026rsquo;t like face-to-face or phone-based conversations. I am much less efficient at talking than I am at writing and am much more likely to make mistakes when talking to someone than when I have a bit of time to think about what I want to say. Email, the ultimate in asynchronous communications, is possibly my favourite way of communicating as it gives all parties involved the headspace to give a thoughtful response. There is nothing more irritating than a one word or one sentence email such as:\n lol thanks\n Or\n Can you help me with something?\n Or even the dreaded\n :-)\n This kind of vapidity has little place in email for me. I tend to get quite frustrated with this kind of thing, so I actually welcome an instant messaging solution as a nice in-between for these types of communication. People can have quick and meaningful exchanges on these platforms which allow them to better determine whether they can resolve what they need to resolve now or whether they should use another form of communication.\nHowever, I cannot abide Slack.\nSlack represents the worst of office communications wrapped up in a frustrating, ugly, and resource-intensive application. I dread having to open it every morning, and whenever I do have to open it to respond to the message I\u0026rsquo;ve just been informed about via email (figure that one out) it sounds like my poor work MacBook is about to take off to fight the Germans above Britain.\nBut Slack is not just technically frustrating: it represents a dangerous trend away from what is important in the workplace. Most messages outside of my team come in the form of \u0026ldquo;hilarious\u0026rdquo; GIFs and links to local news articles with humorous titles. That these exist in the same space as live communications about downtime and discussions about bugfixes just feels very wrong to me.\nSlacking off I am of the belief that people need to stop filling their time at work. If you have nothing to do for whatever reason, you should be allowed to stop working and do something else. After all, you are being paid and not doing anything for the company either way, so you may as well be allowed to go and do whatever.\nHowever, that is not the world in which we live and as such we see a lot of \u0026ldquo;look busy-isms\u0026rdquo; in the workplace. Nowhere is this more prevalent than on the company Slack channels. Slack\u0026rsquo;s all-too-eager-to-please-non-technicals nature has led to it being bundled with a whole deluge of distracting, network-intensive and irritating add-ons which not only have nothing to do with the majority of jobs but are downright distracting when they pop up.\nSlack also has, of course, a whole plethora of very useful features, though I am yet to see them properly utilised in a busines environment. Threaded messages, for example, are designed to reduce the clutter in the main chat and allow conversations to be kept together out of the way. But this isn\u0026rsquo;t how other messengers such as Facebook Messenger and Skype work, and as such these see limited adoption from (I would guess) the majority of users. I\u0026rsquo;ve seen the /gifs integration get a lot of hammer, though. Very useful feature.\nNow, I\u0026rsquo;m coming off as a curmudgeon here, but please understand that I\u0026rsquo;m not trying to say nobody should have fun at work. What I\u0026rsquo;m trying to say is that playing about on Slack when you have no work to do is not only counterproductive in the long-term (making it seem as though everybody is always working is a sure-fire way to keep people in offices for long days because clearly we all have a lot to do), it\u0026rsquo;s also downright irritating for the rest of us. I often just quit Slack to A) silence these notifications and B) save my poor, poor Macbook from the threat of Electron. Then I find out I missed an important message. Damnit.\nUse better As I said above, I am generally a fan of instant messaging. I make use of Matrix every day and spent the majority of my youth on IRC. To this day I sit most of the time in chatrooms be they for discussing software production, offering support, or shooting the shit with my old university pals. I don\u0026rsquo;t want to see IM disappear from the workplace, but I certainly want to see Slack pulled.\nSlack is expensive. At nearly six dollars per user per month it\u0026rsquo;s a really bank account-unfriendly way of allowing coworkers to share gifs and vapid articles. It\u0026rsquo;s also (brace yourselves) non-free software.\n The crowd gasps in shock and horror\n Yes, I know, it\u0026rsquo;s not a concern to most. But be forewarned: Slack is a variety of ransomware. Its free trial allows you to make full use of its features before it starts trying to extort you to get your data back. If you run a business, do not give your data to these thugs. Keep your data safe and accessible by using open alternatives such as Matrix, Mattermost, or even the venerable IRC. You can\u0026rsquo;t go wrong with the classics.\n","permalink":"https://sporiff.github.io/posts/say-no-to-slack/","tags":["tech","chat","communication","rant"],"title":"Say no to Slack"},{"categories":["blog"],"contents":"I\u0026rsquo;ve been working with and enjoying technology most of my life. As such (and given the long history of vendor lock-in and associated cost/challenge with going without the support of major tech companies), I have a pretty significant footprint with certain tech companies. I\u0026rsquo;ve been with Google/Gmail/Android for many years. I have files stored up on Onedrive and Google Cloud. I have been working with Windows, ChromeOS, and even macOS and iOS at various stages as long as I have been working with computers. After some self-education I\u0026rsquo;ve come to the conclusion that these dependencies must end and have been on a path to rid myself of external influences.\nThis is an ongoing list of issues I\u0026rsquo;ve come across and alternatives found. This is by no means a comprehensive list; this is more just a few examples of alternatives I\u0026rsquo;ve been able to fill the holes with.\nCommunication It\u0026rsquo;s an unfortunate reality that communication platforms are among the hardest things to abandon. After all, your friends and family have probably come to depend on them despite telling you years ago that such channels of communication were \u0026ldquo;not as good as a phone call\u0026rdquo;. Facebook, Twitter, and others know this all too well, and try to buy up the landscape around them in order to ensure that you can\u0026rsquo;t go too far outside of their sphere of influence even if you \u0026ldquo;cancel\u0026rdquo; your main account.\nI\u0026rsquo;ve cancelled Facebook at least twice at this point. The first time was a few years back after a mental break that saw me re-evaluating how I interacted with the internet in general. More recently, though, I simply decided to leave. I don\u0026rsquo;t like Facebook as a company or as a product, so it seemed silly to stick around using it. Twitter was a little harder to give up on, but eventually I moved away in favour of my own Mastodon server.\nFor chat, Matrix seems to be the ideal candidate to take over from proprietary solutions. I tend to use this with Weechat, but the more glossy clients such as Riot and Fractal are also available for people not so inclined to use a terminal application.\nProductivity Productivity is generally less of an issue in my experience, as productivity tools are almost invariably superior in the free software world and it is not a requirement that everybody use the same monolithic tools in order to work with each other in every case. I personally can get most of what I need to do done with Vim, Pandoc, LaTeX, and git.\nThe area with which I do struggle sometimes is audio/video production. While there are some excellent tools for these, I have come across quite a few issues with hooking equipment up to a GNU/Linux powered machine and not having access to its full range of features. It\u0026rsquo;s a shame, as it\u0026rsquo;s not the fault of the software developers as much as the OEMs. Most things are designed explicitly for Windows or macOS in these fields, but it is encouraging to see so much work going in to it on the GNU/Linux and *BSD worlds.\nEntertainment Gaming on Linux has improved significantly with the release of Steam\u0026rsquo;s Proton. However, while this is an impressive piece of technology, it is nonetheless keeping you locked in to a specific company and denying you the freedom to do with your games as you will. Luckily, I don\u0026rsquo;t play games on my computer much.\nI\u0026rsquo;ve been able to replace Soundcloud/Spotify/Google Play Music with Funkwhale and have enjoyed being able to run this with the mopidy-subidy plugin on my machine.\n","permalink":"https://sporiff.github.io/posts/going-free/","tags":["tech","surveillance","open source","free software"],"title":"Going free"},{"categories":["reviews"],"contents":"Ever heard of Void Linux? Me neither until a little while ago. My friend had originally introduced me to Void when I was looking at going back to Arch Linux a few years ago, stating that it was (in many ways) a vastly superior system. I was intrigued, but I ultimately ended up sticking with Arch as I was familiar with it and was unwilling to give up on using the AUR at that point.\nFast forward to a few years later and I\u0026rsquo;m back in the market for a new distro to try out. My Librebooted X200s was running Debian testing happily, but I was unhappy with the sheer amount of resource the system was taking to run given the machine\u0026rsquo;s relatively limited capabilities. This was largely due to running GNOME, a DE which I used to have a very low opinion of until recent releases made it a much more pleasant experience. So I decided to give a tiling WM a try once more.\nInitially I was going to go straight back to Parabola running LARBS (which, as I\u0026rsquo;ve previously stated, is my favourite computing experience thus far), but since the creator of LARBS had recently been testing a new ricing script for Void I decided it was a good opportunity to give the distro a try.\nInstallation The installation of Void Linux is surprisingly simple. I was using a terminal-based installation, but the wizard is still easy enough for a relatively inexperienced user to follow. As with most installations of this type, disk partitioning is the most difficult bit to nail. Once you get this, however, the system boots without complaint and remembers everything you had set up, which is still something I find pretty neat. YMMV as I\u0026rsquo;m sure the myriad GUI installs are even easier, but suffice to say this was not as difficult an install as some.\nxbps is Void\u0026rsquo;s homegrown package manager. It functions somewhat similarly to pacman, with the distinction of using multiple commands rather than just using flags.\nUpdating the system # Pacman sudo pacman -Su # xbps sudo xbps-install -Su Installing a package # Pacman sudo pacman -S \u0026lt;package-name\u0026gt; # xbps sudo xbps-install -S \u0026lt;package-name\u0026gt; Removing a package # Pacman sudo pacman -R \u0026lt;package-name\u0026gt; # xbps sudo xbps-remove \u0026lt;package-name\u0026gt; As you can see, the two are syntactically very similar, but xbps is not actually a valid command, which takes some getting used to. In addition, you have to make sure to run xbps-install -Su multiple times after the system is first installed in order to get everything synced properly.\nSetting up the rest of the machine was largely achieved using LARBS, so there wasn\u0026rsquo;t much to get wrong. Just download and run the script and the defaults should take over. Again, YMMV depending on what setup you\u0026rsquo;re going for. The installer will take care of the DE and other general programs, but for the sake of minimalism I decided not to go with that.\nPerformance Void is pretty quick. It boots a lot faster than Parabola/Arch in my limited experience with it, taking about 6 seconds from initial boot to login screen, and DWM is capable of running pretty much everything I need with very little resource. To this point, the highest I\u0026rsquo;ve managed to push the memory is around about 500MB with 10 surf windows, Weechat, and various other terminal programs running. Mostly it hovers around 200MB, which is crazy impressive.\nManaging services with Runit is also really straightforward. Programs with daemon capabilities get added to a directory and enabling them as a service is a simple matter of symlinking the daemon programs to the Runit directory. This means it\u0026rsquo;s incredibly easy to make your own services with just a few scripts. Is it any easier than systemd? Not really. It\u0026rsquo;s definitely lighter, but it remains to be seen if it is as robust in day-to-day usage. I can honestly say I haven\u0026rsquo;t missed any of systemd\u0026rsquo;s 900 billion features at this point, so in terms of a laptop setup systemd is proving to be largely overkill.\nPackages As is possibly expected, Void\u0026rsquo;s package selection is limited. Many distros with established package managers can get away with a limited first-party repository as the rest of the packages will be provided elsewhere, but with Void being the only distro (that I know of) using xbps, it\u0026rsquo;s really up to them to provide packages of everything. All of the most important stuff seems to be accounted for, and most of the time the free software I want to install can be easily obtained, however there are a few notable omissions which need to be obtained elsewhere. This isn\u0026rsquo;t really a problem since you can simply install guix and use it alongside xbps for package management.\nOther package formats such as appimage seem to run perfectly fine, while things like snap are simply not going to work due to snapd\u0026rsquo;s ludicrous hard dependency on systemd. I\u0026rsquo;ve not bothered with flatpak yet as I\u0026rsquo;ve not had a need to yet, but it is included in the repos so it\u0026rsquo;s good to know it\u0026rsquo;s there as a fallback (although given that I\u0026rsquo;m aiming for a minimal system I\u0026rsquo;m going to try to avoid it).\nxbps also comes with an additional option called xbps-src, which is offered as a script in a git repo. While xbps-install will install binary versions of software from Void\u0026rsquo;s repos, xbps-src allows the user to save the source of these packages and build them from source locally instead. Using a git repo to sync these packages feel pretty natural, and the idea of having the option to build from binary or source in a supported way is pretty neat. xbps-src also gives the user the option to package software themselves using xtools to generate templates, which should make it easy for developers to make their software available through xbps in future.\nFinal thoughts I have to say I really, really like Void Linux. I wouldn\u0026rsquo;t say it\u0026rsquo;s on par with Arch Linux for me yet, but I will say it\u0026rsquo;s one of the most solid options out there for anybody looking for a minimal or systemd-free distro. It recalls for me my first experiences using the venerable Slackware, with many of the choices that are usually made for me being put back to me but in an easy-to-configure way. The system\u0026rsquo;s lightweight feel (largely achieved through dwm I\u0026rsquo;m sure), the simplicity of the init system, and the surprising robustness of the package manager makes this a distro I\u0026rsquo;m really glad to have tried. I\u0026rsquo;ll probably stick with it for a little while and see how it treats my little Librebox.\n","permalink":"https://sporiff.github.io/posts/into-the-void/","tags":["blog","tech","linux","computer"],"title":"Into the Void"},{"categories":["blog"],"contents":"I\u0026rsquo;ve made a few changes to this website. Well, not really a few changes, more like a complete overhaul. I was getting a bit bored of using Hugo/Markdown and decided to challenge myself to the following:\n Move to Sphinx Enable Ablog Move away from Github to Gitlab Get Netlify CI working with the new setup  This wasn\u0026rsquo;t too difficult in theory, more time consuming (what can I say? I get bored). It involved taking all markdown files, converting them to rST, setting up Sphinx and all related env variables, creating new build instructions and ultimately getting everything to build seamlessly from the push.\nGitlab Previously I had hosted this setup on Github for ease of use, but recently I\u0026rsquo;ve been finding myself liking it less and less. I\u0026rsquo;ve had a Gitlab for a while and have run my own Gitea server, but it seemed wasteful to keep a server like that running when my personal site (the project I most frequently push to) isn\u0026rsquo;t even pushing from there. I\u0026rsquo;ve therefore decided to retire Gitsune and move operations to Gitlab for now. Eventually I\u0026rsquo;ll probably resurrect Gitsune as a Gitlab instance but looking at the spec for it I\u0026rsquo;ll probably hold off until I have a bit more of an income…\n","permalink":"https://sporiff.github.io/posts/moving-to-sphinx/","tags":["blog","site","website","sphinx"],"title":"Moving to Sphinx"},{"categories":["blog"],"contents":"I\u0026rsquo;ve talked about it before… And it\u0026rsquo;s finally here! After much head scratching, frustration, and eventually the building of an entirely new server, my Matrix server garappa.chat is up and running. This has been something I\u0026rsquo;ve talked about doing for a while as I try to get away from all bigger services in favour of hosting my own. While Mastodon and Funkwhale where considerably easier to set up, I have a feeling it will be worth it in the end.\nThis was a fight For as long as I\u0026rsquo;ve used Matrix I have used the flagship server due to how prohibitively difficult it seemed to be to set up a synapse server Having tried setting the server up as both a Docker container and a native set of applications on the same server as the other two services, I eventually conceded that I was going to need a clean sheet for this one.\nSo to DO we go to spin up a new VPS for my new whim. As I sat about getting the server itself ready for yet another Docker fight I decided to do some reading on other people\u0026rsquo;s struggles with Synapse and what could be done about it. The results were pretty discouraging, with many people hitting quite significant dead-ends and being unable to fix the issue before their patience ran out. However, while looking into other server options I chanced upon a certain Ansible Playbook…\nHaving never used Ansible before I started doing some poking around. In my first run I tried running it from the server as I was pretty much clueless when it came to what Ansible was or how it was supposed to work. After a while, though, I started to see some results. With each new parameter passed I was seeing features added. With one last push this morning I finally got S3 media uploads working, which should hopefully help some with storage costs. Now I want to use Ansible for everything. Whoops!\nLess exciting news With all that excitement here is a bit of less important but still fun news: this website will be written in rST instead of Markdown. The reason for this is trivial, but still important. rST is a proper standard and a really pleasing way to write documents. I can be confident that what I write in one place will render the same elsewhere in a way I can\u0026rsquo;t always be with Markdown. MD is still one of my favourite tools and I use it extensively at work, but for now I\u0026rsquo;m going to focus my efforts more on mastering reStructuredText for my work with the Funkwhale project\nA worthy note As an aside, I have been having a fantastic time interacting with the FW community. A more supportive group of people I couldn\u0026rsquo;t wish to know. The project keeps exceeding my expectations and the people involved show a real interest in improving the community and the level of interaction, which as a novice programmer is really encouraging to see.\n","permalink":"https://sporiff.github.io/posts/matrix-is-here/","tags":["matrix","chat","tech","federation"],"title":"Matrix is here"},{"categories":["blog"],"contents":"It\u0026rsquo;s been a little while Every time I have a blog I am always really bad at posting to it consistently. Actually, a lot has changed in recent months so I thought I\u0026rsquo;d do a little brain dump.\nFunkwhale Thanks to the lovely people at Funkwhale I\u0026rsquo;m finally getting involved properly with a software project. To date I\u0026rsquo;ve provided a couple of new (extremely simple) features and bugfixes, as well as leading a large project to restructure the documentation and provide a slew of new user guides. I\u0026rsquo;ve also tried to get involved closely with the community and maintain an active role even when I can\u0026rsquo;t necessarily contribute anything material to the project. Over the last week or so this has taken a back seat, however, due to…\nHouse hunt My partner and I are looking at buying our first place. Currently we are renting, but renting in Exeter is a painfully expensive and unrewarding experience. We\u0026rsquo;ve found a flat in the centre of town that we both really like, and are now going through all the fun of trying to work out what a mortgage is and how we get one.\nIt\u0026rsquo;s not fun.\nWe\u0026rsquo;re uncertain at this point whether we\u0026rsquo;ll actually be able to complete this move, but we\u0026rsquo;re pretty hopeful that we can move swiftly enough to grab it while it\u0026rsquo;s on the market. If we can, we\u0026rsquo;ll be a lot closer to all of the things we\u0026rsquo;re currently missing out on.\nWork I\u0026rsquo;ve been making leaps and bounds at work recently. A lot of the projects I\u0026rsquo;ve been leading have been directly inspired by my work with Funkwhale, and I\u0026rsquo;m happy to say that the projects to manage documentation in Markdown + Git as well as the process of managing mapping changes in Git have both been officially implemented and are being used across the board. It\u0026rsquo;s been a bit of a fight, but it\u0026rsquo;s been really good to see people get excited about the benefits a project is going to bring them.\nThat\u0026rsquo;s really it for now. It\u0026rsquo;s been a crazy busy month this month (not least with our insane political atmosphere at the moment), but hopefully something really good will come of it eventually.\n","permalink":"https://sporiff.github.io/posts/new-developments/","tags":["funkwhale","blog","personal"],"title":"New developments"},{"categories":["blog"],"contents":"Making a difference It feels good to actually have a positive impact on something. One of my biggest gripes about working in support is that you are – as a support agent – inevitably the face of disappoinment and/or failure at some point. The job is time and attention demanding, and sometimes you simply have to tell someone that things they want are not possible.\nThis is devastating for morale. It\u0026rsquo;s something I\u0026rsquo;ve seen in all my support jobs: the tickets keep coming and all you can do is try to answer them all in SLA. It\u0026rsquo;s a hosepipe of negativity which can really get you down after a period of time. Occassionally, you\u0026rsquo;ll get to the bottom of a particularly nasty problem and really make somebody\u0026rsquo;s day better. For the most part, though, it\u0026rsquo;s a thankless job with a lot of disappointment and no real goals to work towards.\nTurn it around In my previous roles I have tried to find pragmatic and practical ways to lessen the negativity of the support role by using my unique frontline position to directly address the gripes and grievances of customers and coworkers alike. During my time as a frontline hardware repair worker/desktop support agent I spent all my spare time automating small tasks to try and take some of the pain away from the mundane, everyday jobs. I took this approach with me when I was promoted to systems technician, using my newfound access to simplify and automate many of the things which had caused me misery on the frontline so that it never had to be that bad again.\nMy current role is slightly different. For the first time in my professional career I am working in a company that makes something, which means that every decision and project has to be worth something to the product as a whole. No more frittering away idle hours on support desk trying to fix bits and pieces. The changes have to have a tangible impact.\nOne of the reasons I landed this job was because of my background in English, as well as my background as a technical writer and creator of documentation in my previous roles. As such, I was asked to look at the company\u0026rsquo;s help center, which is (still) in need of some updates. As each day passed I started to pick up on something from the tickets:\nNobody read our documentation.\nWe have a lot of documents, mind you. I can\u0026rsquo;t expect every client to read every one. But the fact that the same issues came up again and again said to me that something fundamentally needed to change. The documents were obviously not clear or were frustrating to navigate. Similarly, no centralised process existed to write documentation nor a central place to put it.\nSince I am a big fan of markup languages like Markdown and reStructuredText, I decided to make use of Markdown when writing documents for my own ease, from which I would then copy the pure HTML to put into our document store to be formatted directly by CSS so that when the time came for a redesign, everything could be changed in one fell swoop. I started storing all of these markdown files with their corresponding HTML in a folder structure that reflected the current layout of the help centre. And that\u0026rsquo;s when I realised how we could fix it.\nYou Git I use Git for a lot of things. I write small projects, I contribute to some open-source projects, and I host this website on Github. It is an elegant way to control large numbers of changing parts in a controlled and centralised way. Given that much of what I write goes through Git, I started to put together a proposal for making use of our repository to host documentation. This way – I reasoned – we would have a portable document set which could easily be recovered elsewhere, a set which had perfect revision history and the potential to put in requirements for submission, and a way to work both on and offline with no issues.\nThe biggest bugbear we have at the moment is screenshots. Because the software evolves so rapidly (and will only get faster) keeping screenshots up-to-date is arduous. We have to open each document on Zendesk and manually replace the image. Rinse and repeat. To remedy this, I decided we should use Amazon S3 (which hosts a lot of our stuff already) and keep static links to files which could be swapped out whenever a new version was ready.\nHaving been given the go-ahead by the CTO, I am currently in the process of porting and rewriting our entire document store. It will take a long time, but I\u0026rsquo;m hoping that it will have the following effects:\n Customers will be more likely to use our nice-looking and accurate documents Writing documents will naturally become part of our workflow regardless of what part of the company we work in  Here\u0026rsquo;s hoping!\n","permalink":"https://sporiff.github.io/posts/progress/","tags":["funkwhale","blog","personal"],"title":"Progress"},{"categories":["rants"],"contents":"Mixed messages Okay, so I\u0026rsquo;m slightly reneging on my earlier post by writing about computers again, but trust me when I say this one is as much personal as it is technical.\nBack in t' day It seems so bizarre to me that people are taken aback by my not having a Facebook account. Or a Twitter account. Or anything that they have bar email.\n I\u0026rsquo;ll add you on Facebook. How do you spell your name?\nC-I-A-R-Á-N, but I\u0026rsquo;m not on Facebook\nWhy not?\nToo many reasons\n At this point in the conversation, I\u0026rsquo;m pretty much the asshole, putting them out terribly by not being instantly available on the network of their choice in order to organise impromptu events. But I remember, as I\u0026rsquo;m sure many do, when the world was separated by the internet as opposed to joined by it. I think I preferred it this way.\nSeparated? To clarify, the Internet has always been a great communication tool, and it\u0026rsquo;s certainly true that it has vastly simplified comms over distance to the point where it\u0026rsquo;s unthinkable now that we once waited days for messages to move across geography that now takes only milliseconds to traverse. But the internet used to be a world unto itself, in a way.\nWhen I was a child, I spent long hours browsing the web, sitting in chat rooms and looking at strange and interesting sites (of which this is still the oddest). Then, when I was done on the internet, I could spend time with friends and family. There was a clear separation there: I know you online, I know you offline. You who I know online I shall never meet. You are abstract and faceless, yet just as human as anyone else. You who I know offline are formed and tangible, and I have the freedom to spend time away from you with my faceless chums should I want to.\nBut what do we have now? Once the barrier for entry to the web was lowered, we saw adoption grow rapidly. This is, in a way, a great thing. It meant that people who previously struggled to make use of the benefits of the web now had access to the same powerful tools as those used by web users in days past. But this also had the unintended effect of erasing the barrier between the online life and the offline life. The friends and family from whom I could escape when life became too much suddenly bled into the last fascinating, untouched world I held dear. As more and more people came online, so too did the IRC rooms empty and the strange sites give way to centralised, controlled sites that allowed interaction across all experiences.\nHonestly? I hate this.\nThe internet now is a shell of its former self. Sites like Facebook and Twitter have so centralised, standardised, and sterilised the method and means of online interactions that it feels like all the adventure and fun has been sucked out of the platform entirely. When you do find an interesting looking place, it\u0026rsquo;s often either a ghost town or filled with ne\u0026rsquo;er-do-wells who are more interested in subjecting you to bile and vitriol than getting to know you. The online space is now no more interesting than a high school, and has many of the same problems. Is there a solution? Well, federated services give it a fair crack and IRC is still as enduringly addictive as ever. But the network effect is real. Once a site like Facebook takes hold of the popular psyche, you are being difficult if you don\u0026rsquo;t use it.\nIf I\u0026rsquo;m being truthful I am happy to keep my online life and my offline life completely separate, never interacting with anybody I know AFK online and keeping them in the real world where they belong. But this has the unwanted result of making you a \u0026ldquo;voluntary\u0026rdquo; outcast, who chooses not to be involved in things because you refuse to just give in and join [insert social network here]. Well, ain\u0026rsquo;t that just shit?\nAnyhow. Just a rant. As you were.\n","permalink":"https://sporiff.github.io/posts/bring-back-my-internet/","tags":["funkwhale","blog","personal","rant","internet","federation"],"title":"Bring back my internet"},{"categories":["blog"],"contents":"A fond farewell So for the last few weeks I\u0026rsquo;ve been exclusively using GuixSD on my X200s. It\u0026rsquo;s been an interesting experiment, and there are a good many things I really liked about the system. For now, though, I am going back to Parabola.\nGuix For those who don\u0026rsquo;t know, GNU Guix is a package manager which makes use of Guile to offer a really hackable and minutely controllable software management experience. GuixSD is a GNU/Linux-Libre distro based on this package manager, making use of Guile Scheme all the way down to the system configuration to make your build entirely reproducible. It\u0026rsquo;s a very interesting experience, not without its challenges, and one that I would certainly be interested in trying again in future.\nWhy leave? Essentially, GuixSD is still very much beta software. Important meta packages don\u0026rsquo;t yet work right, certain DEs are unfinished and buggy, and quite frequently I would find my computer simply crapping out on me and shutting down entirely. Since this machine is really just a hacking machine, I wasn\u0026rsquo;t too bothered for the most part, but I decided I\u0026rsquo;d rather have the stability of a well-known system until Guix reaches maturity.\nI also had issues wrapping my head around parts of Guix administration. Due to the immutable nature of Guix stores, it is not possible to edit stores or files once they are built. This means that the quick edits and on-the-fly changes of config files need to instead be carefully thought-out and planned, lest you find yourself spending time rebuilding and reconfiguring. For all of Arch/Parabola\u0026rsquo;s flaws, very little configuration is needed once you get it set up.\nSo for now I\u0026rsquo;m back on Parabola running LARBS, which is still my favourite GNU/Linux experience. I\u0026rsquo;m pretty comfortable and don\u0026rsquo;t see a need to change. However, I\u0026rsquo;m still going to watch Guix keenly to see how it progresses.\n","permalink":"https://sporiff.github.io/posts/goodbye-guix/","tags":["guix","guixsd","gnu","linux"],"title":"Goodbye, Guix"},{"categories":["blog"],"contents":"And now for something completely different I\u0026rsquo;ve been writing posts on this website for a little while now, mostly relating to my technological exploits and sysadmin misadventures. While this is a big part of my life at the moment, it\u0026rsquo;s not the only thing I do. I\u0026rsquo;m going to try and write a bit more about non-technology things just to try to encourage myself to focus on other things alongside computers.\nWhy? Basically, computers have started to take over my life. I go to work every day and work with computers, writing more SQL than I am comfortable with. I administer my Funkwhale and Mastodon servers, which – while something I very much enjoy doing – takes up a lot of my time and money since I am still working it out and am paying for it off my back. I also tend to come home and hop straight on a computer to do… something. There\u0026rsquo;s not always a goal. Sometimes I just kind of mess around on 4chan or Reddit for a few hours every night before going to bed, achieving absolutely nothing in the limited amount of time I have free after spending far too long at work.\nI have other interests that I\u0026rsquo;m keen to get back into. Chief among these is music, and I\u0026rsquo;m looking to get back into recording and posting music up on Tanuki Tunes as a way of marrying up my passions. Another interest is writing. I\u0026rsquo;ve started brainstorming ideas for a short story I\u0026rsquo;d like to write, but I\u0026rsquo;m a pretty piss-poor writer when it comes to sticking to an idea, so I won\u0026rsquo;t hold my breath on that. The last – and easiest of these to achieve – is reading. As a child I was a voracious reader, but as time has gone on I have simply stopped reading. It\u0026rsquo;s not that I no longer enjoy it, it\u0026rsquo;s more that I find myself simply not bothering because it\u0026rsquo;s much more quickly gratifying to waste time on the computer every day. This is something I guess I just need to balance better, as I don\u0026rsquo;t want to lose interest in computers at all, but it can\u0026rsquo;t be my only pastime.\nI\u0026rsquo;m probably going to post more ramblings here as it\u0026rsquo;s an easy and safe way to rant about things in a more thought-out and constructive way than Mastodon. I used to very much enjoy having a blog. I should probably try to enjoy it again.\nI\u0026rsquo;ll be making changes to this site over the next few days.\n","permalink":"https://sporiff.github.io/posts/more-writing/","tags":["funkwhale","blog","personal","website"],"title":"More writing"},{"categories":["blog"],"contents":"Another Funkwhale update After a little bit of frustration on my side due to not following the directions properly, Tanuki Tunes is now running Funkwhale 0.18.1. The changes in this version will be mostly invisible to end-users, but include important bugfixes such as one that prevented the editing of tracks in the Django admin panel. This update has also made me aware that the site was previously running an out-of-date version of Postgresql. This has now been rectified, so the site is completely up-to-date with suggested specs.\nA sad update As many of you were probably aware, this weekend was the weekend of FOSDEM out in Brussels. Yours truly was supposed to be in attendance, however a drizzling of snow shut down all local airports and forced me to give up trying to leave the country after trying for more than 24 hours. I\u0026rsquo;m going to be catching up on the videos from the conference over the next few days and look forward to attending next year (hopefully!)\nOther stuff This past month has been steady for both Baku Social and Tanuki Tunes, with the only downtime being due to the upgrade to the latter. This affected Baku Social due to my lack of experience working with Postgresql and Docker, but it\u0026rsquo;s fair to say I\u0026rsquo;ve learned a lot from the process. Hopefully this will go smoother in future and the two sites should be spun up and down separately from one another at all times as is the point of running them from Docker.\nI am still awaiting the final sketches for Baku Social\u0026rsquo;s new mascot. I\u0026rsquo;ve been working with the artist to make something that reflects both Mastodon\u0026rsquo;s fun, cartoonish vibe and the mythological presence of the Baku. It should be great once it\u0026rsquo;s done!\nTsukumogami and FolkloreThursdayBot have been working well this week with the exception of a double-posting bug with FTBot which I need to look into. Once that\u0026rsquo;s solved, I\u0026rsquo;ll be looking in to other content to make bots for. Any suggestions are welcome, just PM me on Mastodon.\nLastly, I\u0026rsquo;ve updated the theme for this site just for a change of pace. I\u0026rsquo;ve also made a dedicated channel for any music I post, all of which can be streamed directly on the site from Tanuki Tunes thanks to a new embed feature included in version 0.18 of Funkwhale.\nThat\u0026rsquo;s all for now, folks!\n","permalink":"https://sporiff.github.io/posts/tanuki-tunes-update/","tags":["funkwhale","federation","tech","mastodon"],"title":"Tanuki Tunes update"},{"categories":["blog"],"contents":"Technical updates So it\u0026rsquo;s been a bit of a busy month for Baku Social. The server experienced some minor downtime at the end of December owing to high memory usage, however there doesn\u0026rsquo;t seem to have been anything breeching the limits since then so fingers crossed there is no need yet to upgrade the specs of the VPS itself. I\u0026rsquo;m continuing to monitor it to ensure that nothing goes overboard, but given the low traffic both it and Tanuki Tunes seem to be managing fine for the moment.\nOn the server front I\u0026rsquo;ve done some work around stripping down AWS storage and have tweaked the cloud firewall to try and keep as much unwanted traffic out as possible. Things appear to be stable at the moment on this front, so I\u0026rsquo;ll keep this setup rolling as long as it\u0026rsquo;s suitable.\nContent updates Some exciting news on the content front. Having previously been given permission by Matthew Meyer to activate Tsukumogamibot, I have also been given the go-ahead from the editors of Folklore Thursday to activate a daily RSS feed bot: Folklore Thursday Bot Hopefully this will allow me to build up the visibility of both Baku Social itself as well as a couple of projects I\u0026rsquo;m a big fan of.\nIn other content news I\u0026rsquo;ve been in talks with Brenna Stones to provide some commissioned art for the site. I should be able to share this soon and am very excited to have a unique hero/mascot image to differentiate the site a bit from the stock design.\nThat\u0026rsquo;s all for now! As always, you can follow me on Mastodon for updates.\n","permalink":"https://sporiff.github.io/posts/baku-social-news/","tags":["mastodon","tech","federation","social"],"title":"Baku Social news"},{"categories":["blog"],"contents":"I\u0026rsquo;ve been looking for a project recently to teach me some more about various technologies, specifically public cloud stuff like AWS and container tech like Docker. I have been looking to get into this stuff for a little while and even previously hosted on of my blogs on a container OS, but that was shortlived due to too many headaches with Wordpress containers. For the last few months I had also been looking for a way to replace Spotify – which I had left behind due to privacy concerns, costs, and library limitations (I listen to some… weird stuff). So when I heard about Funkwhale I decided this would be a good opportunity to give the whole Docker/Cloud thing a go.\nA rocky start Initially I opted to run the entire setup in AWS with an EC2 server and additional storage to host media files. This went well to begin with; setting up the server, DNS, and access to the service was very simple. I ran the site for a couple of days on this setup before realising that costs would quickly spiral out of hand. While AWS is fantastic for large-scale applications and deployments, it\u0026rsquo;s not great for smaller projects with lower specs. The storage and network costs would quickly grow beyond what I was comfortable spending, and so I decided to call it a day with EC2 and fall back to my tried and true: Digital Ocean.\nI opted originally to run a $40/month VPS with 8GB RAM, 4 cores, and 160GB SSD. This seemed logical seeing as I was going to be hosting media files on the server. At the time, I had placed my entire uncompressed library on the server, so it was taking up about 100GB of space just to host the files. This didn\u0026rsquo;t bother me too much as the server spec included a lot of bandwidth as well, so I plowed ahead. I got nginx up-and-running, set Docker up successfully, and was easily able to listen to music out of the Funkwhale app. Very nice.\nBut this server is… expensive. And very powerful.\nMy biggest mistake in this move was picking a high-power VPS for its disk size rather than more sensibly choosing a less powerful machine and attaching an additional disk to it. As such I was now stuck with a whole lot of server hosting, well, my music library and nothing else. I didn\u0026rsquo;t really have the room to add more music in future, and the server was too powerful to just have one Docker container running on it.\nMy solution? Find another project of course! I have been a longtime user of Mastodon, having moved away from sites like Twitter and Tumblr a while back due to toxicity and privacy concerns (again). While I was happy enough using Fosstodon and Mastodon.technology before, I was keen to start my own instance dedicated to Japanese mythology (my first true love). With the idea in mind, I looked up the guides and — after a few teething issues — had an instance running successfully.\nAround this time I decided that I probably needed to downsize everything. The server was still too powerful and I had been stupid in my allocation of storage. So I decided to spin up a smaller VPS with half the power and a 100GB dedicated drive, compress all of my music, and migrate the whole thing over. After a brief conversation with the Funkwhale dev I was able to get the media root changed and have all music switched over to the external drive. Bonza.\nThe end result The two federated instances are now successfully up-and-running. The Mastodon instance is Baku Social and the Funkwhale is Tanuki Tunes. I\u0026rsquo;m still monitoring the server, S3 storage, and general performance to see if the setup I\u0026rsquo;ve chosen is right, but so far I\u0026rsquo;m not seeing any particular issues coming up. All seems to be smooth sailing. I guess my big challenge will be managing upgrades and downtime, but I\u0026rsquo;m yet to experience migrating to a new release.\nThese two projects are currently being managed, hosted, and paid for entirely by me. I don\u0026rsquo;t mind this too much as I\u0026rsquo;m currently the only one using the services so they\u0026rsquo;re basically just my personal playing ground. However, I have also set up a Patreon and a Liberapay to help cover costs should other people want to make use of the services.\n","permalink":"https://sporiff.github.io/posts/baku-social/","tags":["mastodon","social","tech","federation"],"title":"Baku Social"},{"categories":["reviews"],"contents":"It seems like you can\u0026rsquo;t use anything these days without spending a good amount of time trying to keep up with changes to the product you\u0026rsquo;re using. My partner frequently complains about apps on her phone being updated, only to find that entire sections of the UI have changed with seemingly no warning. It is a frustration, to be sure, and following mailing lists, forums, and Slack (grr) groups to keep up with everything is an exhausting experience, but it is also something that is becoming more and more necessary as we enter the age of the rolling release. One such change I\u0026rsquo;ve become aware of having set up my first WordPress site in a few years is the switch to Gutenberg in the upcoming WordPress 5. Never one to be left behind on such advances, I installed the plugin and started playing around with the \u0026ldquo;future of WordPress\u0026rdquo;.\nInitial thoughts It is immediately obvious to anybody who has used WordPress for a long time that Gutenberg is a radical departure from the venerable tinymce which has been our WordPress companion for so long. The simplicity and straightforward-ness of the simple text box has been replaced entirely with a \u0026ldquo;clean\u0026rdquo; white sheet, which invites the user to enter a title in a \u0026ldquo;block\u0026rdquo;. More on this later…\nIt feels obvious to me that Gutenberg is greatly inspired by medium\u0026rsquo;s approach to the WYSIWYG editor. From the fixation on modular design, to the white sheen of the background, even down to the design of the \u0026ldquo;add block\u0026rdquo; button. Again, none of these things are negatives, just observations. Well, except the which background. That is something that really needs to be killed off by UI designers (nota bene this writer suffers from vision problems and white backgrounds are the bane of his existence).\nBlocks and modular design Unlike the more traditional markup-style WYSIWYG editing we are used to experiencing with tinymce, Gutenberg splits everything on the page up into \u0026ldquo;blocks\u0026rdquo; – modular sections which are defined by their content. Rather than jumping right in and writing a paragraph with a header, you first specify that this first block will contain a header, while the follow-up block will contain paragraph text. I can follow the logic here, and it isn\u0026rsquo;t a particularly frustrating process. This allows the writer to very neatly manipulate the different elements of the page.\nBut is anything really being gained by splitting everything up like this? Using tinymce, for example, writing a blog post was not dissimilar to jumping into an office program such as LibreOffice Writer and plugging away. Gutenberg adds steps which easily allow the writer to convert blocks from one type to another, certainly, but it wasn\u0026rsquo;t like it was difficult to do that before.\nThe range of the blocks is perfectly acceptable. One feature I can actually see being quite useful is the ability to add blocks which are purely HTML and have them sit seperately from the rest of the text without needing to switch back and forth between visual and HTML editing.\nBlock and document settings In keeping with the modular design, the settings on the right-hand side of the editor are split into two segments: one for the document as a whole and one for the currently highlighted block. The document settings allow the author to do the usual tagging, scheduling, and obligatory post options, while the block settings allow for some additional customisation options such as drop caps, text size, colourization, etc.\nIs there a tangible benefit to this? Well, not really for anybody experienced with working in the HTML editor. I can, however, see it allowing non-HTML users to access some features with a lot more ease. From a UX perspective, it\u0026rsquo;s comfortable and intuitive in a way that perhaps tinymce was not. It neatly and cleanly allows the user to compartmentalise the design of their post without getting confused by a wall of text and buttons. I, for one, had no problem with tinymce\u0026rsquo;s approach, but neither do I have a problem with Gutenberg\u0026rsquo;s approach.\nA few teething problems As this is still essentially a beta product, one can expect there to be some initial problems with it. For example, on my blog I have been unable to render a preview to check that everything is working as expected. Adding images to a paragraph after typing it up is a bit more difficult than it perhaps should be (you need to create a new image block and then nest it within the paragraph to achieve the effect seen in the first paragraph, for example.\nThere does seem to be a fair amount of hostility towards this new product. Many users are following the old \u0026ldquo;if it ain\u0026rsquo;t broke, don\u0026rsquo;t fix it\u0026rdquo; adage, while others are pleading with Automattic to keep it as a plugin rather than integrating it with WordPress Core in version 5.0. But here\u0026rsquo;s the thing: Gutenberg actually does address an issue with WordPress. While there is nothing functionally wrong with tinymce, don\u0026rsquo;t forget that WordPress is facing stiff competition from the likes of Medium. Why is Medium so popular? Well, besides its tight social integration it has a very clean, pleasant-to-use and intuitive editor. WordPress would soon find itself languishing if it continued to stick to tinymce.\nSo what\u0026rsquo;s going to happen going forwards? Realistically not very much. Tinymce will still be available in WP 5.0, and Gutenberg will probably be enthusiastically enjoyed by newcomers to the platform. Once again, I think there is a certain extent to which complaints are being drummed up for the sake of workflow familiarity. People get so used to working in a certain way that any change or disruption to this can cause panic. But really, that\u0026rsquo;s the way the world is going. Part of your workflow needs to be adapting to change.\n","permalink":"https://sporiff.github.io/posts/getting-on-with-gutenberg/","tags":["wordpress","tech","gutenberg","writing"],"title":"Getting on with Gutenberg"},{"categories":["blog"],"contents":"So I\u0026rsquo;ve been saying for a number of years that I\u0026rsquo;d really like to get into app development, but have never really had a project to work on. This is usually my struggle. I am the sort of person who learns by doing, but until I have a need to do something I can\u0026rsquo;t bring myself to learn.\nFollowing on from my previous post I have been using the latest release of elementaryOS for the last few days and having a poke around at their documentation. I\u0026rsquo;ve been really impressed by the relative simplicity of approach applications seem to have on the OS and the documentation. Having followed through a bit, it looks like the ideal platform for me to actually try and make my first app on.\nThe app will not be complicated (obviously). The idea is to combine two short programs I\u0026rsquo;ve written previously: a simple Vala/GTK app written along with an elementary guide and a C program I wrote a while back to translate temperatures between °C, °F, and K. Since I already have the basic calculations written out for this, I don\u0026rsquo;t need to worry too much about the maths (which is my greatest foe).\nInstead I want to focus on UX and UI, specifically with regards to how the app deals with input and output of text. I know exactly what the app to look like and how I want it to work, the trick is going to be tying together all the bits and pieces while also trying to teach myself enough Vala + GTK to make it look like it does in my head.\nThis is just a post jotting down thoughts and the like. Obviously, this is going to take me a while since I\u0026rsquo;m completely lacking in experience. Just thought I\u0026rsquo;d post something lest I forget to do the damn thing.\nAs a quick update, I\u0026rsquo;ve added thermy to my Sourcehut as (very much) a WIP. As the app is updated when I have time to spare I\u0026rsquo;ll push it straight there. Wouldn\u0026rsquo;t expect too much, though.\n","permalink":"https://sporiff.github.io/posts/thermy/","tags":["code","coding","programming","program"],"title":"Thermy"},{"categories":["reviews"],"contents":" Note: I have contributed translations to the Elementary OS project and have been an active Patreon supporter since April 2018. I don\u0026rsquo;t think of myself as particularly biased, but it\u0026rsquo;s best to mention these things\n Summary (for those who don\u0026rsquo;t want to read) I like Elementary OS. It does plenty right and offers a minimalist system with decent keyboard control, a healthy environment which encourages users to invest in developers, and it largely keeps out of my way while offering the power of Ubuntu under the hood. The system is not without its bugs (which I expect to see ironed out as reports come in), and there is still a little too much in the system for my liking personally. However, the vision of Elementary has been realised no clearer than in this release. I won\u0026rsquo;t be covering everything in this post, just some initial thoughts and reactions. See the official release notes for a better technical rundown.\nThe main meat In an earlier post I expressed my preferences in a desktop operating system: simplicity, lightweight operation, and easy customisation. It may surprise some (and not surprise others) that I am a big fan of Elementary OS. Though the project is vastly different from the barebones and stripped-back OS I prefer, I have a lot of respect for the \u0026ldquo;Elementary Vision\u0026rdquo; and have actively participated in its community as a patron and translator for the past year. They recently released their fifth official version, Juno. This is not an in-depth review, but more a cursory glance at the latest version of this system as a returning user.\nInstallation Process As one might imagine, installation of Elementary is as simple as most desktop-oriented GNU/Linux distros these days. Plug in a live USB, choose to install, and follow the instructions. Given that I was coming from an Arch Linux install, this process was quick and easy. It took about 15 minutes from start to finish and at the end (though I had to initially remember to unset the nouveau modeset) I was able to boot into the system with no issues. Quick, easy, unobtrusive, as expected really.\nI was slightly surprised to see the Ubuntu installer being used as I seem to remember some blog posts about improving the installer to push people towards disk encryption, but apparently while this has been added to Pop! OS it\u0026rsquo;s still not in Elementary. Strange.\nPantheon Everybody who has looked around the GNU/Linux distro landscape in recent years has probably come across Pantheon at some point. They most likely thought \u0026ldquo;oh hey that looks like macOS\u0026rdquo;. Indeed it does. It\u0026rsquo;s a lot more eye candy than I\u0026rsquo;m used to. However, as somebody who works with macOS a lot, let me tell you now that I much prefer the way Pantheon works.\nUnlike macOS, which has an interface which constantly begs for the user\u0026rsquo;s attention, Elementary OS' Pantheon knows to stay the hell out of the way when the user is trying to do something. Screen real-estate is kept relatively free and interactions with the desktop part of the system are kept to a minimum so you can focus on doing what you need to do.\nJuno has had a lot of work put into keyboard controls using super key commands. As you probably know, I\u0026rsquo;m a massive fan of this way of working. The super key brings up a list of keyboard shortcuts by default, which will be really helpful (I hope) in convincing more people to join the keyboard master race.\nVirtual desktops really leave a lot to be desired still for me. By default, two desktops exist: the first is the active desktop and the second is a blank. Hitting super + left/right will take you between these two, but until you add apps to the second screen you cannot add a new desktop. Hitting super + down reveals all windows and desktops, allowing for easy rearrangement, but this whole feature feels far too much like macOS for my liking. Being able to preset a number of blank desktops for easy navigation is preferable. Once the desktops are generated you can move between them easily using Super + Number. Still that damned animation, though. Fortunately, this can be turned off along with all other animations in the accessibilty panel. See, Apple? That\u0026rsquo;s how you do it!\nWindow tiling has seen vast improvments in this release with good, easy to control snapping and resizing. This is a very welcome addition. However, the snapping animations can be a little buggy when trying to move windows from left to right. The animation wants to follow a nice cycle of left, fullscreen, right. This is a minor nitpick, but I would want this much \u0026ldquo;snappier\u0026rdquo;.\nNotifications have also seen a few improvements (particularly in sound design), but by far my favourite thing thus far has been the ability to alter the type of notification per application. This is really handy for when you have apps which demand your attention (Mail) and ones which really do not need to be making noise when an event happens (Music). It\u0026rsquo;s a little thing, but I\u0026rsquo;ve found Juno to be full of little things that make a difference. They have, as they say, chosen all the door handles in the house.\nThe Elementary team has also won my heart with their inclusion of dark themes in many system apps, including the newly introduced Code (previously Scratch) in which I am writing this post now. It\u0026rsquo;s a decent IDE. I would recommend giving it a try. In dark mode, of course. And speaking of Code…\nApplications Probably my favourite thing about Elementary OS vs. most other distros is their attempt to encourage the funding of developers. This is a somewhat controversial opinion, I think, as many people don\u0026rsquo;t like feeling \u0026ldquo;guilted\u0026rdquo; into paying money when the software could be free. However, as somebody who cannot code to save their life, I am happiest when I can invest a small amount of money as a thanks to developers who continue to make my life easier. Elementary puts this idea first and foremost in their AppCenter, which offers a pay-what-you-want feature on certain Elementary-specific applications. One gripe I do have with this is their attempt to enforce the paying of developers by disabling the ability to bulk-update for feature releases if you haven\u0026rsquo;t paid for the software. Not only does this feel a little restrictive, the fact that the store does not store what purchases have been made means I will now have to purchase certain apps again in order to have them bulk update. Seems a little premature.\nThe system apps follow a nice design pattern, with everything that comes with the system feeling like it belongs there. I am a little disappointed that the dark theme hasn\u0026rsquo;t found its way into more of the Elementary core apps, however. I feel like a dark theme would really complement mail and music. For now though, everything definitely feels like it belongs and despite a few exceptions such as communication apps I\u0026rsquo;ve found little need to step beyond the core, which is exactly what I want in an OS. No I don\u0026rsquo;t care about office software. I have Vim and Code to write LaTeX and Markdown.\nOn this note, I have to go back to Code for a minute. There are some upsides and downsides to Elementary Code. The major downside is that the removal of Scratch leaves users with no standard plain text editor, which will lead many needing to download Gedit or an equivalent. But Code itself is a really promising program. A simple enough IDE with nice look and feel, the addition of extensions such as Vim keystroke emulation (hallelujah!) and web previewing make this feel like a really comfy app to sit in and hack on. I know Linux hackers are picky about their IDEs, and Code is nowhere near as extensible as the venerable Emacs or Vim, but I still feel like giving praise where it\u0026rsquo;s due here.\nA personal experience I just wanted to include here: I downloaded a Hacker News reader made for Elementary called HackUp. I was pretty impressed, but disappointed in the lack of a dark theme. I left a feature request on the project\u0026rsquo;s GitHub and by the next day the dark theme was implemented. Well worth the $3 any day.\nThis not only speaks to the developer community into which Elementary is investing, but also to the clarity and conciseness of the system\u0026rsquo;s UI guides and stylesheets. If an app is made for Elementary OS, it is made for Elementary OS. Many people would see this as a bad thing, but I\u0026rsquo;m not really that bothered provided the software is free. You can always fork and tweak if you want to.\nOverall I personally feel that Juno is a big step up from Loki and a huge step forward for the Elementary team. The OS feels very polished (despite a few bugs, which I\u0026rsquo;m hoping to see ironed out fairly soon) and it has a cohesive design the likes of which cannot reliably be found in many other distros. Software support is good, the system is small and unobtrusive, and the addition of a payment system for developers is a welcome addition.\nIs this OS for everyone? Absolutely not. No Linux distro can possibly be for everyone. That\u0026rsquo;s kind of the beauty of it.\nNever forget that Windows is the result of an operating system trying to be all things to all people; it simply does not work and ends up pleasing very few. Elementary OS knows its audience and it knows what to focus on in order to reach that audience. It feels like a system with a coherent vision and goal, and if that matches up with your ideal it\u0026rsquo;s a really good place to start.\nOh, and the British English translations are truly excellent. Hats off to whomever did those.\n","permalink":"https://sporiff.github.io/posts/elementary-juno/","tags":["gnu","linux","elementary","distro"],"title":"Elementary OS Juno"},{"categories":["rants"],"contents":"Let me get this off my chest right now. I hate Apple. I hate iOS. I hate macOS. I hate the software, the hardware, and the ethos of Apple. To my mind, Apple represents the very worst of the technology industry: devices which are hard to use, easy to break, and difficult to fix. Therefore, I\u0026rsquo;m never surprised when I see news stories like this one.\nThe long and short of this story is that Apple have implemented a new chip (the T2 chip) which can detect whether or not repairs to a broken machine have been carried out by a verified technician using a pre-configured pin code. If a user tries to get their machine fixed by somebody outside of Apple (or a verified reseller, I hope), their machine will be essentially bricked. Until they pony up the repair money, of course.\nAs I say, this move is entirely unsurprising. I first started using macs back in 2009 when I completely fell for their marketing and, desperate to be rid of Windows and \u0026ldquo;different\u0026rdquo; from those around me, I plonked down £899 for a MacBook Pro with pitiful specs. The machine had the following:\n A 160GB HDD 2GB of RAM A Core 2 Duo 2.26Ghz processor  For £899. Jesus. What an idiot I am.\nNow, I was assured (as are all iDiots) that the hardware was unimportant as macOS (OSX at the time) was so superbly optimised that it could run circles around a Windows machine with twice the on-paper specs. When it came to using the machine, however, I can report that this is an absolute falsehood. The machine ran okay for some tasks. But whenever I deigned to do any more than open a web browser and a couple of other basic programs, it sounded as though I was sat on the runway at Heathrow during a particularly busy holiday period.\nI have bought the machine primarily for film editing and music production, as these were my two key hobbies as a youngster (heck, I even studied film at university). And sure, the editing programs for both media were straightforward and loaded with presets, but render times were abysmal and — quite frankly — Apple\u0026rsquo;s handling of Audio is far worse than that of FreeBSD or GNU/Linux using JACK. But, like most people who had been duped out of close to £1,000, I stuck to my guns and defended the little MacBook. I upgraded its RAM (the only component I could afford to upgrade) and hoped for the best.\nBut then something happened: the machine started to fall apart. I don\u0026rsquo;t mean \u0026ldquo;it started to perform poorly\u0026rdquo; I mean it literally started to fall apart. The first thing to go was the screen, which started to peel off from the bottom. This was in the days before the screens had an aluminium bezel, kids, and the frontispiece was almost entirely cheap black plastic. Alarmed by this, I booked it in with Apple to repair, which they did under my AppleCare.\nIt took about 2 weeks.\nWhen I got it back I quickly got back to using the machine regularly, annoyed that I\u0026rsquo;d had to go back to that accursed Windows for the time in-between. Happy as Larry, I kept plugging away using the machine like… a laptop.\nThen the hard drive failed.\nI got it repaired after a week.\nThen the disk tray failed.\nAt this point, it was apparent even to my then 16-year-old mind that I\u0026rsquo;d bought into a scam. This machine, sold to me as a luxury item worthy of a near £1,000 charge, was more flimsy than the atrocious Acer Aspire it had replaced. It was slow, expensive, frustrating, loud, and completely unfit for purpose. After 3 years of use, I cut my losses and returned to the world of Windows1.\nThe right to repair So what does this little story have to do with the recent news? Quite simply, one of the best things about general purpose computers is that they are infinitely repairable. The best models (usually old IBM/Lenovo ThinkPads) are very modular and allow for the replacement of the majority of components by anybody with a toolkit and a bit of knowledge. At the very worst, you can usually call out a trained mechanic and have them repair the machine at a much lower cost than the OEM.\nBut Macs have always been different2. With Apple, it always feels like they\u0026rsquo;re trying to pass off their slow and frankly shoddy repair service as exclusive by making it necessary to always use technicians they verify. Hard drive busted? Get Apple to fix it. Yes I know you can literally just hotswap it in other computers but this isn\u0026rsquo;t like other computers2. This computer was crafted by ye gods for only the most unique of humans.\nMac hardware is flimsy and easy-to-break by design. The machine upon which I am currently typing (a MacBook Air my job requires me to use) feels like it could fall apart at any moment. The trackpad and keyboard are flaky, the hinge feels loose to the point of being concerning, and the USB ports are temperamental at best. Why would you design a machine to work well over long periods of time? That would lead to people A) not paying you for warranties and B) keeping their old machines longer. It makes no business sense.\nBut this latest move is a real kick in the teeth for the industry as a whole. Before, you could take your machine to Apple or a reseller for major repairs, but small things like replacement keys and screens could be easily and cheaply repaired by yourself or an independent repair worker. This new move makes that an impossibility and is utterly anti-consumer. Watching people defend it is hilarious and heartbreaking (I was there once, too, don\u0026rsquo;t forget).\nWhat to do? Apple is not the only offender here, even though they are the most egregious. PC manufacturers and Android phone makers are similarly locking down devices and making repairs difficult and risky. If you own a modern Lenovo ThinkPad (particularly models such as the X1 Carbon), you are going to have a much harder time getting in to repair things when they break. This means purchasing additional warranties is usually necessary because it feels — at least to me — that machines these days are much more likely to break.\nI was at FOSDEM in February and was — well, not exactly surprised, more pleasantly surprised — at the number of hackers walking around carrying old X and T series ThinkPads from 2008 and beyond. The people making a lot of the software that powers the world often don\u0026rsquo;t see the need to go further in terms of processor modernity3, opting instead for superior repairability and more efficient software. I have found that in my interactions with older machines running less graphically intensive software can perform the majority of day-to-day operations with as much ease as any modern laptop.\nBut hey, you know. MacBooks are shiny.\n  During my university years I encountered many people using Macs (after all, I studied film) and always felt slightly smug when I could render things at a fraction of the speed using my beastly and far cheaper PC. \u0026#x21a9;\u0026#xfe0e;\n They tend to crash differently, at the very least. \u0026#x21a9;\u0026#xfe0e;\n An important note is that modern processors — while faster — are not so much faster that they need to be snapped up while leaving perfectly functional models behind. The major difference between models like i3, i5, and i7 lies in their graphics capabilities, not their processing power. The Core 2 Duo is actually a perfectly reasonable processor for the majority of people if they don\u0026rsquo;t use a ridiculous number of graphically intensive programs. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://sporiff.github.io/posts/apple-drink-cider/","tags":["apple","rant","t2","mac","tech"],"title":"Apple: drink the cider"},{"categories":["rants"],"contents":"This post is more of a rant than anything technical. In fact, it\u0026rsquo;s more of a moan born of recent frustrations. Nevertheless, this is my website. Where better to moan?\nI\u0026rsquo;ve recently started a new job working for a software company. My day-to-day work mostly includes rifling through SQL databases and making changes as well as searching for causes of issues faced by customers. This job has me using a work-issued MacBook Air and a copy of Windows 10 running in a virtual machine. It\u0026rsquo;s the setup used by every employee of the company bar one tester, who gets to use GNU/Linux.\nI hate this.\nI am a particular person. My home PC runs Arch Linux with LARBS as a meta-distribution which I have further customized to my liking. I am very happy with my setup. It allows me to zip around with comfortable and functional keybindings and to spend less time trying to do something and more time actually doing the thing. This is my machine. This is my workflow. macOS has a very different workflow with neither works nor flows in my opinion, but it\u0026rsquo;s a work machine so it\u0026rsquo;s really just a minor gripe.\nThe other day, my partner asked whether she could use my computer to check on something. I said \u0026ldquo;of course\u0026rdquo; and launched a web browser for her. Within a couple of seconds I registered the confusion, then annoyance, and then anger in her face as she tried to grapple with the unfamiliar system. When I offered to look it up for her, she just said \u0026ldquo;why don\u0026rsquo;t you just get a proper computer?\u0026rdquo;\nNow, she has her own laptop which runs Windows 10, though I am yet to actually see her use it for anything. That is her computer and it works in a way that she finds at the very least familiar. This comment, however, made me realise something: she, like so many others, has never used a personal computer.\nWhat I mean is this: when a person installs an operating system like Windows, macOS, or virtually any distribution of GNU/Linux, they will pretty much stick with the system exactly as it is set up. Rather than fix things they don\u0026rsquo;t like, they will simply get used to them and then complain when they change. I know because that\u0026rsquo;s exactly how I am most of the time. It wasn\u0026rsquo;t until very recently that I started thinking about exactly what it is I need a computer to do that I realised I was going about it all wrong by using default interfaces. I needed to address what wasn\u0026rsquo;t working for me and learn how to rectify it.\nA short journey I became inspired to start looking into my workflow after reading a book called Clean Code by Robert C. Martin. In particular, the assertion that good code makes it look as though the language was made explicitly for the creation of the programme stuck with me. When I look at my work Mac (upon which I am currently writing) I dont feel like it was made for me. I don\u0026rsquo;t feel like Windows was made for me. I honestly don\u0026rsquo;t feel like any one distro of GNU/Linux was made for me, either. I just used them because I didn\u0026rsquo;t like macOS or Windows.\nThe first step I took was to address exactly what was wrong. My MacBook gave me my first hint at some of the issues I needed to address in my daily life:\n Things on computers are unnecessarily slow. Transitions between workspaces on macOS are frustratingly sluggish. Mouse navigation is pointlessly inaccurate and tedious. I need to get rid of the mouse and any annoying transitions. OK. But when I started looking around for ways to disable these obnoxiously slow animations, my results were threadbare and the result was just a way to \u0026ldquo;reduce motion sickness\u0026rdquo;1 and so the speed issue remains. Keyboard control on both macOS and Windows is pitiful. So much focus is put on the mouse that navigating the OS without it,while possible, is like fighting a honey badger in a pool of molasses. I need to have more control with the keyboard because I find mouse navigation annoying. macOS Mojave introduced a dark mode recently, as did Windows 10. However, both of these things are useless if the stock apps used are still light and unable to be changed by system-level themes. Web browsers and office programs are particularly at fault here. So I need to do away with both wherever possible.  This realisation led me to ricing boards and eventually to Luke Smith, whose video demonstrating him shaving a few milliseconds off a BASH script greatly impressed me. His setup looked largely ideal, and since he is kind enough to distribute it, I put Arch back on my machine and gave it a shot. After a few hours with the manual I was able to go through everything and make the changes that made it feel more sensible to me. The whole process took me just one evening with the right base and mindset.\nWhy does this matter? So back to the point at hand. My setup is the result of wanting my computer to do better for me than a default OS setup will do. Given the opportunity to sit down and simply look at what I was doing with a computer I now have a setup that allows me to spend less time on the computer simply because I have got everything done much faster than I would have done. Can other people use my computer? Sure, but they would have to work how I work for it to make sense.\nComputers are, at this point in time, being produced for all people to use2 and as such are tailored to nobody. Worse yet, systems like Windows and macOS really don\u0026rsquo;t give you the tools you need to make your experience better. It\u0026rsquo;s a bit like walking into a hardware store looking for a screwdriver and all they have is hammers. Sure, you can get the work done, but it will be inefficient and ugly. But imagine if you were given a toolbox from which you could pick. Given a couple of hours of work and reading, you can wrangle BSD and GNU/Linux to be just that.\nI would encourage more people to have personal computers. I think the idea of each computer being set up specifically for its user while all software works interoperably so that ideas can still be communicated is a wonderful notion. What we have at the moment is total homogeny and a societal contempt for anybody who defies it. Mac users are the worst of the bunch in this regard, accepting Apple\u0026rsquo;s vision of workflow unflinchingly and unquestioningly. I should know, I was/am one by necessity. But Apple\u0026rsquo;s workflow is horrendous, designed to keep you on the computer rather than allow you to complete work with the computer. The distinction is important and becomes abundantly clear once you move yourself over to a system which you have tailored to your needs.\nIf a computer works exactly how you expect it to, ask yourself why you expect it to work that way. Is it because it\u0026rsquo;s what makes the most sense to you, or is it because it\u0026rsquo;s simply what has been presented to you before? Think about what frustrates you while you use it and how you might address those problems. You may be surprised at what you come up with.\n  By the by, Apple, you have utterly failed at design if your system is known to cause motion sickness by default. \u0026#x21a9;\u0026#xfe0e;\n Although I\u0026rsquo;m sure many people with disabilities would disagree \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://sporiff.github.io/posts/personal-matters/","tags":["rant","tech","pc","personal","computer","gnu","linux","windows","mac"],"title":"Personal matters"},{"categories":["blog"],"contents":"As the astute among you may have noticed, this site has undergone some changes over the last couple of days. That\u0026rsquo;s because I\u0026rsquo;ve now completely moved away from Wordpress and destroyed my old server in favour of a different provider.\nThere are a couple of reasons for this:\n The server was costing me money that I didn\u0026rsquo;t have to spare Wordpress is slow and cumbersome. Even a simple site like this was taking far too long to load I wasn\u0026rsquo;t learning anything by using Wordpress  This last point is the reason I\u0026rsquo;ve moved to the solution I have. I\u0026rsquo;m now using Netlify in conjunction with GitHub to host my site and am writing everything in Markdown to be generated using Hugo. Why? Well, let\u0026rsquo;s take each of these points in turn.\nFirstly, I\u0026rsquo;ve been thinking about getting away from Wordpress for a while now. Secondly, while I have a few repos on GitHub, I am not exactly a git aficionado and really need to learn more for future reference. Thirdly, writing posts in WYSIWYG editors is just kind of a crutch at this point. Now that I\u0026rsquo;m spending more time getting to know Vim and LaTeX for home use, I feel like getting to know Markdown at the same time is a good idea. Lastly, I am just as guilty as many others of running a simple site with far too heavy an interface. There is no justification for a blog such as mine to run on WP when static pages will suffice.\nI\u0026rsquo;m also going to be focusing more on RSS as this is a technology I would dearly like to keep alive. I know not everybody likes it, but as somebody who is finding themselves more and more living in a terminal it is an essential thing for me.\nThat\u0026rsquo;s all for now. Hopefully this move isn\u0026rsquo;t too jarring. If you prefer light themes, the lightbulb in the top right will switch you over. This site is dark by default because it\u0026rsquo;s better for your eyes, yung\u0026rsquo;uns\n","permalink":"https://sporiff.github.io/posts/new-site/","tags":["tech","blog","markdown","fun"],"title":"New site"},{"categories":["blog"],"contents":"Righto. This one has given me a mild headache for the last couple of days, but I\u0026rsquo;ve found a workable solution that allows me to set a home page for users in Chrome. You would have thought that would be really easy, right? Well, Google in its infinite wisdom has decided that conventional Windows management is for wusses. So down the rabbit hole we go.\nIngesting ADMX templates Let us say, for the sake of argument, that you have deployed the Chrome Enterprise .msi to your devices either during a build or using Intune. Now you want to control some of the settings for your users in order to provide a consistent experience. If you look around online, you\u0026rsquo;ll see various references to ingesting ADMX templates and leveraging these for using with Chrome. This looks something like this:\n# Create an OMA-URI to import your ADMX template * ./Device/Vendor/MSFT/Policy/ConfigOperations/ADMXInstall/Chrome/Policy/ChromeADMX} * Data Type = String * Value = the whole body of the Chrome ADMX template # Leverage policies from within this template using nested OMA-URI settings * ./Device/Vendor/MSFT/Policy/Config/Chrome~Policy~googlechrome/ShowHomeButton * Data Type = String * Value = \u0026lt;enabled/\u0026gt; Pretty simple stuff. This allows us to show the home button. Tickedy boo. So what about setting the home page and landing pages for users? Well, unfortunately, this isn\u0026rsquo;t possible.\nWhen you use the CSP settings to load and edit the GPO locally, Chrome detects that the computer is not connected to an on-prem AD, and will ignore certain settings including but not limited to your custom home page. Bum gravy. This is mentioned in some capacity in this thread and in this advisory, but it\u0026rsquo;s far from a satisfactory explanation in an Azure AD connected world.\nWhat is a boy to do? So here\u0026rsquo;s the dilemma. I know that another way to control Chrome is using a file called master_preferences located in the C:\\Program Files (x86)\\Google\\Chrome\\Application folder, so for future deployments it would be entirely possible to simply place this in the image. However, we have nearly 60 devices already out in the world due to the unreal pressure we\u0026rsquo;ve been under to deliver, so I need a remote deployment method.\nFirst option is to use Powershell. Powershell could easily be pointed at a publicly accessible Azure blob containing the master_preferences file. The problem with this is that Powershell scripts can only be targeted at users, which would mean that in the long run we would not be able to set different preferences on different types of machines.\nSo, what about a .msi file? I got this idea in my head to package the file up as an application, which would give us the power not only to deploy per device but also to supersede if we ever want to make changes to the config. The two tools I used to achieve this were NSIS and the free version of exemsi. The steps are as follows:\n Edit your master_preferences file to reflect the changes you want to make Place the file in a zip archive Run NSIS and select \u0026ldquo;Installer based on ZIP file\u0026rdquo; Select your ZIP file, give the installer a suitable name, and set the\u0026quot;Default Folder\u0026quot; value to C:\\Program Files (x86)\\Google\\Chrome\\Application Hit \u0026ldquo;Generate\u0026rdquo; and locate your newly created .exe file Next, open up exemsi and click \u0026ldquo;Next\u0026rdquo; to start the wizard Select your newly created executable and change the .msi name if desired In the installer options, select \u0026ldquo;Per Machine\u0026rdquo; under \u0026ldquo;MSI installation context\u0026rdquo;. This is required to allow you to deploy to devices as opposed to users. If users are your targets, set this to \u0026ldquo;Per User\u0026rdquo; Give your application and ID and generate an Upgrade Code. This will allow you to uninstall and supersede if needs be, so keep a note of it and use the same code for any new iterations. Intune will notify you if the codes do not match Enter some information about the product including the name, the manufacturer, and the version number Optionally, set up contact links For install and uninstall arguments, use the flag \u0026ldquo;/S\u0026rdquo; (make sure this is capital). This is the standard NSIS silent install flag There is no need to enter any before/after command lines Hit \u0026ldquo;Build\u0026rdquo; at the end and it will generate a .msi  After this, upload your .msi to Intune, deploy it to a test group, and watch the magic happen. Please note that you cannot deploy the ADMX template and the master_preferences file at the same time as the two conflict with one another.\n","permalink":"https://sporiff.github.io/posts/deploying-controlling-chrome-intune/","tags":["tech","microsoft","azure","chrome","intune"],"title":"Deploying and controlling Google Chrome settings using Microsoft Intune"},{"categories":["blog"],"contents":"So you want to use Microsoft Teams in your organisation, huh? Are you prepared for the pain? The lack of documentation? The feeling of utter exasperation at a company unable to properly consider the needs of enterprise customers in their new \u0026ldquo;modern\u0026rdquo; approach? Well, I\u0026rsquo;m here to share my experiences with you so that hopefully you can avoid some of the pain I had to go through in getting this all ready for our upcoming rollout.\nSCCM deployments Like many of you, my organisation utilises Microsoft\u0026rsquo;s powerhouse deployment tool: SCCM. Initially, when Teams was released it was only available as an executable, which is fine. A little jiggery-pokery with detection methods yielded a usable application which could easily be deployed and self-served by users. Unfortunately, deploying the .exe and then trying to switch to the .msi requires the use of a cleanup script so we were lucky that Microsoft – in their infinite wisdom – decided to release the .msi \u0026ldquo;machine-wide installer\u0026rdquo; before we started deploying. Saves us a bit of work.\nThe .msi installer, as you might imagine, works perfectly as an application. The only caveate is that the installer is per-user no matter what you specify in your application settings. The program installs to the users' AppData folder, so it can cause headaches in environments which make use of redirected AppData.\nIntune deployments This is where things got a lot more complicated. Intune is at once a product with a great deal of documentation and one with absolutely no useful information. So when my co-worker and I initially uploaded the .msi to Intune we expected it to work. Nobody else had complained of any issues with the deployment via Intune, so we were surprised when every attempt at installation came back as having failed.\nSo here\u0026rsquo;s the thing: Intune HATES deployments based on devices. Nearly every device configuration profile, compliance profile, and application deployment is expected to be done per-user. This does not fit our use-case, as users need to interact with many different devices in different contexts (e.g. shared user devices and personal devices). For that reason, we\u0026rsquo;ve had a lot of problems getting policies to work and are currently trying to get M$ to fix the issue.\nBut what about Teams? After all, the Chrome Enterprise installer worked fine targeting devices. The error message from Intune was that devices could not be targeted with user-based installers. But Teams is a \u0026ldquo;machine-wide installer\u0026rdquo;, right? Well, no. Not exactly. The installer basically just sets up a base downloader that is initiated by a user logging in. So how do you get Intune to recognise it as a device installer? You edit the .msi of course.\nI personally use SuperOrca to do this, but there are other tools available which can do the same job. Basically you just load up the .msi and add the following value to a new row. This will set the default installation target to \u0026ldquo;machine\u0026rdquo;, and will enable deployment of the .msi to device collections within Intune.\nStopping AutoStart The single most annoying thing about Microsoft Teams is its inability to shut up. By default, the application is set to launch in the foreground at login. It\u0026rsquo;s not a small app, either, so on older machines it increases load up time substantially. When you\u0026rsquo;re trying to get non-techie users to use new tech (teachers, for example) the last thing you want to do is annoy them with clunky, slow applications.\nSo, I started poring over the documentation looking for an enterprise kill switch for this behaviour. An ADMX template, maybe? Doesn\u0026rsquo;t look like it. A native GPO for Teams behaviour? Nope. Hmm. Okay then. Let\u0026rsquo;s brush off Procmon and look at what this is doing.\nSo it looks like when you enable/disable the autostart from within the Teams desktop application it writes and verifies a bunch of regkeys. The one we\u0026rsquo;re interested in is this one:\nHKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run com.squirrel.Teams.Teams C:\\Users\\%currentuser%\\AppData\\Local\\Microsoft\\Teams\\Update.exe --processStart \u0026#34;Teams.exe\u0026#34; --process-start-args \u0026#34;--system-initiated\u0026#34; So, by creating a login script using group policy which searches for and removes this key, we effectively wipe out the autostart behaviour. Now, we have a working Teams deployment on SCCM. As for Intune, it remains to be seen whether or not creating a CSP and installing local GPOs will consistently suppress autostart. To be tested next week.\n","permalink":"https://sporiff.github.io/posts/teams-deployment-headaches/","tags":["tech","windows","teams","microsoft","intune","azure","sccm"],"title":"Microsoft Teams deployment headaches"},{"categories":["blog"],"contents":"For the past couple of weeks I\u0026rsquo;ve been trying out new things. New to me, anyway. And as you might expect, I\u0026rsquo;m really bad at them. Let\u0026rsquo;s talk about that!\nWhy did it have to be Moodle? I work in a very Microsoft-centric environment. The vast majority of our servers are Windows 2016, 2012 R2, and a few straggling 2008 R2 boxes which just refuse to die. Possibly for this reason, we\u0026rsquo;ve never had a GNU/Linux technician. Our head of IT is very Linux-literate, but also time-poor. When I moved from the frontline team to the systems team, then, all the Linux responsibilities immediately fell to me.\nFan-fucking-tastic.\nMy first job with GNU/Linux was to automate patching for the servers and EPOS systems as they hadn\u0026rsquo;t been patched in years. That was all simple enough, just write out a nice little cron job with logging and leave them to it. Much less hands-on than trying to manage WSUS. But of all these servers the two most problematic ones are definitely our CentOS boxes which power Moodle.\nI had no experience with Moodle until this week, but I had some experience with CentOS. Enough to know that these boxes had been set up by a monkey with absolutely no knowledge of how to partition disk space, set up Apache, or do pretty much anything with GNU/Linux. So when it came time to upgrade Moodle, I was pretty confident that we were going to be boned.\nLuckily, we managed to get the upgrade on the test server finished (not without some trial and error), but much of the way it was set up left myself and our Moodle dev scratching our heads with a \u0026ldquo;why the fuck did they do that?\u0026rdquo; kind of confuzzlement. Hopefully, this experiment will translate well when we do the live upgrade in a few months, and then we\u0026rsquo;ll be able to design the whole server-side from scratch and push it into Azure with proper load-balancing.\nSpeaking of Azure So in this post I detailed some of the trouble we\u0026rsquo;ve been having with getting devices into AAD and Intune management using an SCCM build sequence. I have a quick update to that as well. When last we looked at it we had got devices auto-enrolling in AAD but had yet to hand authority to Intune. When we eventually got the auto enrolment sorted (turns out you have to assign authority to the user group in O365 admin panel, not Azure panel. Go figure) we were still having issues managing the devices in any real way. Intune was simply saying \u0026ldquo;MDM status: see configmgr\u0026rdquo;. Hum.\nMy co-worker quickly realised that the SMSTSPostAction I\u0026rsquo;d set up was working in that it removed the CCM client from the machine, but it wasn\u0026rsquo;t fully successful in turning off EVERY element of CCM\u0026rsquo;s authority. While you\u0026rsquo;d have thought that uninstalling the client would take authority away, it turns out that it really just leaves it in a managed state as though waiting for CCM to gain control again. Not ideal. With a bit of research my colleague figured out which regkey to turn off and I started writing the new script.\nThis was a bit more involved than running a simple command line like before. First we needed to change the SMSTSPostAction to call on the PowerShell script from a local directory in bypass mode. Then, as the last step, we needed to map a drive to point to the script\u0026rsquo;s location and use another PowerShell script to copy the CCM removal script to the C:\\Windows\\Temp directory to be executed post OSD completion. The script itself goes like this:\nStart-Process -FilePath \u0026#39;C:\\Windows\\ccmsetup\\ccmsetup.exe\u0026#39; -Args \u0026#34;/uninstall\u0026#34; -Wait -NoNewWindow # wait for exit $CCMProcess = Get-Process ccmsetup -ErrorAction SilentlyContinue try{ $CCMProcess.WaitForExit() }catch{ } # Stop Services Stop-Service -Name ccmsetup -Force -ErrorAction SilentlyContinue Stop-Service -Name CcmExec -Force -ErrorAction SilentlyContinue Stop-Service -Name smstsmgr -Force -ErrorAction SilentlyContinue Stop-Service -Name CmRcService -Force -ErrorAction SilentlyContinue # wait for exit $CCMProcess = Get-Process ccmexec -ErrorAction SilentlyContinue try{ $CCMProcess.WaitForExit() }catch{ } # Remove WMI Namespaces Get-WmiObject -Query \u0026#34;SELECT * FROM __Namespace WHERE Name=\u0026#39;ccm\u0026#39;\u0026#34; -Namespace root | Remove-WmiObject Get-WmiObject -Query \u0026#34;SELECT * FROM __Namespace WHERE Name=\u0026#39;sms\u0026#39;\u0026#34; -Namespace root\\cimv2 | Remove-WmiObject # Remove Services from Registry $MyPath = “HKLM:\\SYSTEM\\CurrentControlSet\\Services” Remove-Item -Path $MyPath\\CCMSetup -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\CcmExec -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\smstsmgr -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\CmRcService -Force -Recurse -ErrorAction SilentlyContinue # Remove SCCM Client from Registry $MyPath = “HKLM:\\SOFTWARE\\Microsoft” Remove-Item -Path $MyPath\\CCM -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\CCMSetup -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\SMS -Force -Recurse -ErrorAction SilentlyContinue # Remove Folders and Files $MyPath = $env:WinDir Remove-Item -Path $MyPath\\CCM -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\ccmsetup -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\ccmcache -Force -Recurse -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\SMSCFG.ini -Force -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\SMS*.mif -Force -ErrorAction SilentlyContinue Remove-Item -Path $MyPath\\SMS*.mif -Force -ErrorAction SilentlyContinue #Remove authority from CCM $MyPath = “HKLM:\\SOFTWARE\\Microsoft” Remove-Item -Path $MyPath\\DeviceManageabilityCSP -Force -Recurse -ErrorAction SilentlyContinue A little lengthy compared to the last one, but lo and behold Intune picked up the new machine and started applying policies. Huzzah!\n","permalink":"https://sporiff.github.io/posts/back-to-the-drawing-board/","tags":["tech","moodle","gnu","linux","centos","azure"],"title":"Back to the drawing board"},{"categories":["blog"],"contents":"It\u0026rsquo;s been a little while since I last posted here. I\u0026rsquo;ve been having a very busy and productive time at work. Let\u0026rsquo;s jump in to some of the stuff I\u0026rsquo;ve learned over the past couple of weeks. Come on! It\u0026rsquo;ll be fun!\nOr not. Who knows? More importantly, who cares what you think? It\u0026rsquo;s my blog and I WILL cry when I want to.\nIt\u0026rsquo;s all about the cloud As we all know, Cloud services are the way absolutely everything on God\u0026rsquo;s green bloody earth are going these days. Microsoft device management is no different. So, of course, with our upcoming move to Windows 10 we need to devise a way to get all of our mobile devices managed by InTune rather than SCCM and Azure Active Directory rather than on-prem AD just so we can ship them out into the big wide world and still keep them managed. This sounds like it should be easy, but when you\u0026rsquo;re working at the scale my job is working at the prospect of joining devices in bulk to Azure cloud services is daunting. Microsoft\u0026rsquo;s official recommendation seems to be going to each device in turn and joining them to AAD/InTune using the built-in wizard, but of course we don\u0026rsquo;t have the time or staff to do that. Let\u0026rsquo;s script it out instead!\nOSD teething issues So you\u0026rsquo;ve built your Windows 10 image and got everything set up just how you like it. You insert it into your task sequence, and start applying all your drivers and software. Unfortunately, you\u0026rsquo;re trying to use Office 365 Pro Plus with shared user activation and have packaged it as an application as per Microsoft\u0026rsquo;s instructions. Well, let me tell you now sonny Jim, that ain\u0026rsquo;t gonna work. While the deployment of this application will actually be successful, you are going to see your builds failing with error code 16389 A LOT when trying to run this in the OSD. What is error code 16389? Fuck knows. A lot of Googling brought up nothing useful.\nNow you\u0026rsquo;re running around like a headless chicken changing every single deployment rule you can think of. When you try installing on a live system, it returns code 0 so clearly it\u0026rsquo;s working. Try it again in the OSD, 16389. Utter madness. I spent a good day and a half trying to work out what the hell could be wrong. The detection method was right, the application worked, the content was accessible and upon completing the build the software was present, but the error code caused the sequence to complete with some problems\nSo how do we get around this? We\u0026rsquo;ve used the wizard and packaged everything up nice exactly how M$ recommends but it\u0026rsquo;s still not working. Well then. Clearly, Microsoft doesn\u0026rsquo;t know what they\u0026rsquo;re talking about. Let\u0026rsquo;s instead build the O365 PP installer as a traditional package with a script installer. Does it work now? You\u0026rsquo;re goddamn right it does. No more software failures!\nEnsuring authority So one thing you\u0026rsquo;re going to come across that can stump you a little is setting up InTune as the authority for software management on your mobile device. See, when you build a machine using SCCM you load a config manager client on to the machine which deals with setting up software and managing communication to the CCM server. However, the presence of this client will cause InTune to reject the machine as it doesn\u0026rsquo;t have the proper authority. Since we\u0026rsquo;re not going down the path of co-management (InTune only for mobile devices), we need to get that off of there.\nNow, the trick with this is remembering that getting rid of the CCM client will ALWAYS cause a task sequence failure. If you insert a script as the last step with the command C:\\Windows\\ccmsetup\\ccmsetup.exe /uninstall you\u0026rsquo;re going to get a whole load of error messages in your trace log and ultimately the build will fail. This is going to cause oddities. We need a smarter way around this. One way would be to set up a Powershell script in the build which sets a scheduled task to run the command after the machine\u0026rsquo;s first boot, but surprisingly there\u0026rsquo;s a more elegant way to do it.\nIf you add a Task Sequence variable during the OS setup with the variable SMSTSPostAction and write the above script in as a command, the machine will store this command to run it upon successful completion of the task sequence. After a bit of testing and tweaking, I started to get successful builds with no CCM client on them at all. All the benefits of the OSD with none of the residuals! Hoorah!\nAzure AD is a bastard Okay, so we know that these machines need to join Azure AD and not on-prem AD. This seems like it should be easy enough, but unfortunately Microsoft are not keen on bulk joining devices like this. Eventually, it looks like they caved under the pressure from (justifiably) annoyed enterprise customers and created a WICD tool to enable bulk enrollment using a ppkg.\nWe used a ppkg to set some of our customization in the build, so we were fairly confident this would work perfectly. Using DISM, we applied both to the image and watched eagerly for our asset to appear in AAD.\nIt never did.\nWe saw a device joining, for sure, but it wasn\u0026rsquo;t anything we recognised. \u0026ldquo;DESKTOP-£\u0026amp;$*£\u0026amp;*\u0026rdquo; something or other. Not anything like our standard naming convention. But it was being joined by the bulk join account. Hmm.\nSo I did a bit of digging around in the registry and found the issue. The machine\u0026rsquo;s ORIGINAL name is DESKTOP-whatever. This value gets changed in the TS but only once you boot in to the live image. Of course, using DISM we\u0026rsquo;re working with an offline image, so it\u0026rsquo;s joining the machine using this old name and then going online. Well that\u0026rsquo;s not good enough. We need to find a way to apply the package using an online image. To cut a long story short, here\u0026rsquo;s how I did it.\nFirst, we add a step in the TS to map a network drive. We point this to the UNC of the folder on our config manager server where the PPKG is stored and we use a login with the relevant permissions to access it so we can be sure it\u0026rsquo;s picking it up. Then, we write the Powershell script to actually install the ppkg:\nInstall-ProvisioningPackage -Path \u0026#34;YourPackage.ppkg\u0026#34; -ForceInstall -QuietInstall Now just package up that script, add a step to run it after mapping the drive. Badda-bing, badda-boom, the machine now joins AAD with the name you assigned in the TS. Hooray!\nBitlocker The next step is to ensure that your machine Bitlockers properly and passes the key back to AAD so that your organisation can recover it if necessary. This is easy enough when you have a device like a Surface which supports InstantGo, but I\u0026rsquo;m not lucky enough to just be working with these high-end devices. Some of our devices are kind of shit, yo. So we need a universal solution. Powershell to the rescue once more.\nThe trick here is the order in which you do things. This Bitlocker step should obviously be placed after the Azure AD Join step we made earlier. The machine will then need to provision a key, back up the key to Azure AD and then encrypt the drive with that key. The script looks like this:\n#Provision the key first and force it to encrypt the drive with the provisioned key Add-BitLockerKeyProtector -MountPoint \u0026#34;C:\u0026#34; -RecoveryPasswordProtector #Create the variable $BLV with the value of the bitlocker\u0026#39;d C:\\ drive $BLV = Get-BitLockerVolume -MountPoint \u0026#34;C:\u0026#34; #Back the key up to Azure for the drive that needs to be encrypted, assigning the drive\u0026#39;s ID BackupToAAD-BitLockerKeyProtector -MountPoint \u0026#34;C:\u0026#34; -KeyProtectorId $BLV.KeyProtector[0].KeyProtectorId #Enable Bitlocker on the system drive (without a login pin) Enable-BitLocker -MountPoint \u0026#34;C:\u0026#34; -EncryptionMethod XtsAes256 -UsedSpaceOnly -TpmProtector Package it, add the step, and move on with your life. The machine will now Bitlocker the used space and back the key up to the item in Azure AD.\nInTune management This last bit is the bit I still haven\u0026rsquo;t got working. As I mentioned before, we\u0026rsquo;re trying to give InTune full authority over the devices rather than SCCM, so what we\u0026rsquo;ve looked at doing is setting up a group within AAD which have device adding rights. As of yet, the devices are not pulling through to InTune despite being owned by the users in that group. From what I can see, we may not be able to get it working that way. However, my suspicion is that if we use CCM\u0026rsquo;s co-management setup during the build but then remove the CCM client as per the steps above, it will enroll the device and remove CCM\u0026rsquo;s authority. More testing is required, but hopefully we\u0026rsquo;ll have got it working next week.\nPhew! It\u0026rsquo;s been a bit of a slog, but a very productive one. Hope someone finds this somewhat useful.\n","permalink":"https://sporiff.github.io/posts/some-more-windows-work/","tags":["tech","windows","sysadmin","sccm"],"title":"Some more Windows work"},{"categories":["blog"],"contents":"A Fun Experiment I mostly seem to hang out with coders. It seems that the people with whom I most frequently interact are computer scientists or software developers. I don’t really know why, other than they are the only people crazy enough to use Riot for their day-to-day communications and chatroom facility. For this reason, I’m surrounded by programming with little to no comprehension of what is going on the majority of the time. Attempts to explain even the most basic things to me usually end up with me staring blankly and nodding my head hoping that nobody realises I’m the idiot in the room.\nOh, I’ve done the usual stuff. Taken some online classes at Codecademy, Pluralsight and udemy, but while I can complete these exercises with relative ease (the principles do actually make sense after a while), I find myself forgetting everything I just learned almost instantly. For me, learning is a process of necessity. Quite simply there is no need for me to learn programming, so I simply haven’t ever sat down to tackle the task as I have with BASH, PowerShell, and the like. But I get the feeling that if I never learn at least one language to some level of proficiency, I will regret it.\nFor this reason, I’ve spent today setting up my IDE with an eye to learning Perl 6. I thought about returning to C, but since most of my friends seem incredibly enthusiastic about Perl I saw no harm in starting fresh with it. My first project will be porting over an old, short program I wrote in C earlier this year and then seeing if I can’t expand on it a little. Who knows? Maybe I’ll even give it a GUI.\nI think I need a break, though. Just wrestling Windows and Atom into a usable shape for this has been an ordeal unto itself…\n","permalink":"https://sporiff.github.io/posts/knit-and-perl/","tags":["tech","coding","code","programming","perl"],"title":"Knit and Perl"},{"categories":["blog"],"contents":"Let’s clear up some FUD At the moment, information on this flaw is scarce, so as always it’s best to wait until the paper is published (which EFF says will be done at 07:00 UTC on May 15th). However, there are some things we can clear up from the start.\nFirstly, it does not look like this is a flaw with PGP itself, but rather with implementations of GPG used in email programs such as Enigmail. Both GPG and PGP remain cryptographically sound from what is being said by security experts, so there’s no need to worry that the sky is falling down. The official GPG Twitter account tweeted the following message:\nThey figured out mail clients which don\u0026#39;t properly check for decryption errors and also follow links in HTML mails. So the vulnerability is in the mail clients and not in the protocols. In fact OpenPGP is immune if used correctly while S/MIME has no deployed mitigation.\n\u0026mdash; GNU Privacy Guard (@gnupg) May 14, 2018  But as always, it is best to wait for the whitepaper to be released.\nSecondly, it’s getting rather irksome that researchers are announcing these vulnerabilities without actually giving the public enough information to work with. EFF, TheHackerNews, and the researchers who broke the story are sending people into a panic with a seemingly hyperbolic description of the issue. Details will be released tomorrow, they say, so why not wait until tomorrow to break the story? Sure, they tell people to stop using email plugins for GPG and switch to Signal, but if this thread on the GPG mailing list is to be believed, there really is no need to do so if users stop using HTML emails or follow precautions such as using authenticated encryption and a MIME parser. This sort of information is much better than telling everybody to cease using the product because everything they’ve ever done with it is fundamentally broken.\nThe real issue here Let’s face it: PGP email sucks. From the information available today, it looks as though the issues are mostly being caused by insane defaults in email plugins, or by a user’s misuse of the system (for which there should be no room by design). As I say at the beginning of this post, there are still vestigial strongholds of PGP emails in some industries, but if this whole debacle is any indication of how things are going with it, clearly these systems are too difficult for the average user to use properly.\nCSO Online is keeping up-to-date with the issue on their blog, and have given a far better breakdown of the issue than I can. At this point, all we can do is wait for tomorrow and see what the responsible parties have to say on the matter. I would say, however, that this disclosure has been handled atrociously. There has already been far too much panic over something which has an unknown impact, and perhaps has already been mitigated in some circumstances. The researchers seem keen to keep the matter under wraps until the release of the whitepaper, so it will be interesting to see what has been missed out come tomorrow.\nUpdate The whitepaper and further details have now been published. As we all know, it\u0026rsquo;s not a vulnerability until it has a website a cute logo.\nSecurity researcher Matthew Green has started a thread on Twitter detailling some of his thoughts on the attack. It\u0026rsquo;s very much worth a read and he does an excellent job of breaking down the vulnerability into layman\u0026rsquo;s terms. Essentially, it looks as though the vulnerability allows attackers to modify encrypted email and add malicious HTML code to it, which is then executed on the receiver\u0026rsquo;s computer, allowing the text of the email to be sent to remote servers. This is quite a glaring problem, and seems to be on the part of the email plugins not detecting the attack and failing to act on it rather than the protocol itself being broken.\nThe thread goes on to elaborate that PGP is less an issue here than S/MIME, a more widely used protocol which is also vulnerable to attack. This should really have been the meat of the story, as PGP is largely used by a small subset of people while S/MIME is trusted in a lot of critical environments.\nThis whole (dreadfully handled) debacle needs to be taken as a wake-up call to the fact that email is an inherently insecure and outdated technology. People scoff when told to use E2EE messengers, but the fact remains that these apps are made for the modern world and are not stuck in the past in the same way as email is. GPG email is an incredibly old idea that has been shoehorned in to desktop mail programs (which themselves have questionable security when compared to their web counterparts), and is one that really needs to be addressed. Email is more important today than perhaps ever before: it\u0026rsquo;s your digital home address, your online home. There needs to be a conversation about increasing security without trying to use overly complex systems such as PGP. And no, making a Tutanota-styled encrypted message for Gmail is not a solution.\nUntil the least technically competent among us can use encryption fluently and without frustration, there will be no security at all. Signal and iMessage have both managed to address this problem expertly, providing strong encryption without letting the user see any of the underlying complexity. We have to remember that the average user of technology is stupid, and that it is on developers and technology experts to make sure they are protected without having to jump through ridiculous hoops.\n","permalink":"https://sporiff.github.io/posts/pgp-problems/","tags":["tech","efail","pgp","mail","security","email","gnupg"],"title":"PGP problems promise pounding headaches"},{"categories":["reviews"],"contents":"I have lived a Windows-free life for a good long while now, using the OS only at work and leaving my home setup free of Microsoft’s influence. However, with the upcoming shift to Windows 10 in my place of work (which I will be helping to develop) and my need to study PowerShell and Hyper-V for my 70-410 exam, I decided to bite the bullet and once again load Windows 10 Professional on to my machine. This is nothing more than a brain dump of my experience on returning. Let it be known that I do not hate Windows as many Linux enthusiasts do; indeed, I’ve been very impressed with many of the advances that Microsoft has been making across their product lines. However, my experiences with it have been patchy at best.\nThe updates, the updates Since I initially bought my Windows 10 license back in 2017, my download was stuck on version 1703, which is a good long way behind where we are now. So, of course, after the long process of installing the OS from USB, I had to let the machine sit for a few hours while it installed updates. I will say, to Microsoft’s credit, this process is far cleaner and less frustrating than before. Not once did I see that accursed percentage wheel which seems to take many moons to budge. Instead, the machine booted and informed me that a big update was needed and that it would perform this in the background using the Windows Update Assistant. This was a much more streamlined and pleasant way to experience the update process, and really highlights how hard Microsoft has worked on it.\nWith that being said, it wasn’t all smooth sailing. For example, I began to set up the rest of the computer while the update was happening: installing my favourite apps, restoring data from backups, etc. etc. I certainly didn’t expect to be kicked off with no warning. Granted, the machine gave me a notification saying that it would reboot, but there was no option for me to postpone this while I finished installing Office. Instead, the machine unceremoniously booted me off for the next hour while it finished installing 1803. Hum.\nWindows Update is much less clunky now than ever before. Yesterday, I moved both my main machine and my partner’s machine back over to Windows 10 with much less fuss than I’ve ever encountered. However, it still baffles me that Microsoft can’t seem to take a leaf out of the Linux book on updates. Many distros have really mastered the art of the smooth update, giving users the option of using a graphical interface with easy-to-dismiss prompts or having complete control in the terminal. I understand that Microsoft is trying to push users into accepting the importance of security updates, but there is something that remains quite frustrating about the whole process. I’m sure it’s going to get better with time, as it is already miles ahead of where it used to be, but I can’t help but miss my apt, dnf, yum etc.\nA slap in the face Anybody who knows me will know that I utterly despise advertising. I reject wholeheartedly the use of adverts on the web to make money, and certainly cannot abide the idea of paid-for service advertising to me atop the money already given. The idea is just anathema to my ideals. For example: if Netflix showed adverts in addition to its monthly bill, I would have to cancel the service. After all, what’s the point of me paying when I’m still going to be subjected to this money-grubbing behaviour? Unfortunately, Windows 10 represents the worst of this behaviour.\nJust to give you and idea of why this upsets me so much, I mentioned before that I don’t usually use Windows. I nearly always load Linux on to any machine I buy out of preference, but it is only since I started getting my machines custom-made that I’ve been able to avoid paying for a Windows license. I should dearly love to see the £90-odd per machine I’ve paid back from Microsoft, but it is a loss I have to accept. What I cannot accept, however, is when a product for which I have paid so many times then has the audacity to advertise to me.\nThe advertising can be pretty subtle, and the majority of it can be turned off by following any of the myriad blog posts on the subject. The issue here is more a moral one, though. I paid for my Windows license. Microsoft has my money. Sure, Windows 10 is a rolling release, but people will still be buying new computers – and thus new licenses – so the whole argument of supplementing income falls on its face at that point. The use of adverts on my computer is indefensible.\nI feel much the same about this as I did about the inclusion of Amazon in Canonical’s Ubuntu, although it was arguably worse in that case as Ubuntu is supposed to be a free OS while Windows very forthcoming about its cost and the fact that the OS ultimately belongs to Microsoft, not you. But worse than the advertising on the home screen and the little surreptitious messages throughout the desktop experiences are the bloody apps I never asked for. Never in my life would I play Candy Crush, much less download it every time I get an update. From my understanding, this “feature” can be disabled through group policy on the Enterprise and Education versions of the OS, but not Professional or any other. Luckily, when there’s a will there’s a registry hack. Some quick Googling led me to the following fix: locate (or create) the following key HKLM\\SOFTWARE\\Policies\\Microsoft\\Windows\\CloudContent and make a new DWORD called “DisableWindowsConsumerFeatures” with a value of 1. This disables the PC’s ability to download new apps that Microsoft thinks you want. But really, this is just utter bum gravy.\nThe rest There’s not much for me to say apart from that, as most people have been using Windows 10 for a while now. It’s certainly better than it was during my time on the Insider’s Preview. It’s stable, it’s polished, and it is much better tuned for my PC’s hardware than most any distro I’ve come across. I have my grievances and my times when I just wish it would work like Linux, but I suppose for that I always have my virtual machines and WSL.\nSome specific areas of note: Edge is now miles ahead of where I thought it would be. It’s now actually usable as a daily driver (though I still use Firefox for most things), and has some really nifty features including a Pocket-like tab saving feature. The Windows Dark Theme really helps a great deal with my eyes, and the night light is really easy-to-use and customisable. It seems like a lot of time and thought has gone in to making the OS more accessible for people with vision trouble, such as myself. This is greatly appreciated.\nWindows Apps from the store seem to be a great deal more usable, too. In particular, Slack has an excellent offering as they’ve managed to simply bring their win32 app to the store as an appx (which is really the only way this store is going to work until more work goes in to the appx format). There is a much greater feeling of consistency throughout the entire desktop experience than ever before. In particular, though, I’ve been impressed with Windows Defender. What was once a complete joke of an AV solution has been able to pick up on issues and provide fixes for problems I didn’t even know my computer had (such as outdated TPM firmware and BIOS upgrades). The interface is clean and easy to use, and it has nowhere near the footprint of solutions such as Avira and Sophos in my experience.\nI may post more on this subject as I continue to use Windows. Most likely, I’m going to be using my experiences to find all the things we need to turn off in our gold image for rolling out later this year. Gulp.\n","permalink":"https://sporiff.github.io/posts/reluctant-return/","tags":["windows","tech","gnu","linux","sysadmin"],"title":"A reluctant return to Windows"},{"categories":["rants"],"contents":"My team recently moved away from a (frankly old and creaking) ManageEngine ServiceDesk solution to Jira for our Helpdesk. This has been met mostly with dismay by the majority of my team, so much so that I am one of the only people in the office still excited about the upgrade. As always, there were teething issues during the initial upgrade, but a few months on everything is more or less stable and we have lots of ideas for how to evolve the product. So why do the rest of the team still bemoan the product so much?\nIt\u0026rsquo;s not what I know This sentiment is not limited to this one situation. In fact, it\u0026rsquo;s a major problem in the tech world in general. Many\u0026rsquo;s the time I\u0026rsquo;ll make upgrade somebody\u0026rsquo;s computer only for them to complain that it\u0026rsquo;s no longer what they\u0026rsquo;re used to. This is a gripe that I really cannot understand. Sure, you get used to working a certain way with a certain piece of software, but if everybody had this mentality technology would never move forward in any real sense. Sometimes, the way you\u0026rsquo;re used to working is provably poor, you\u0026rsquo;ve just become so used to wrestling with it a certain way that relaxing seems uncomfortable and strange.\nIn the case of this example, though, it was more than a simple interface change; the entire way we work had to be reimagined. While we had become used to using ServiceDesk as a glorified email handler which generated jobs to assign to agents, Jira\u0026rsquo;s role in our organisation was always more of a platform than a simple helpdesk. This meant that right off the bat every single action in a technician\u0026rsquo;s workflow required many more steps. No longer were we just sending emails to customers, suddenly we were interacting with automated status changes, jobs which could be dynamically and manually updated on the fly to contain useful information, and a host of addons which gave us additional functionality. As soon as it was introduced, my curiosity led me to do some research on Jira and why our Innovations Officer had brought it in. It quickly became apparent that this was not a replacement or an upgrade: it was a paradigm shift.\nTechnology dragging culture The problem with the way we were used to working really was not our software but more our culture as a team. We had become mostly compartmentalised, with the frontline being entirely separated from the systems team. Jobs would be thrown back and forth across the ServiceDesk in an attempt to close them. JIRA was not a solution to this problem, of course, but more of a symptom of a culture shift. There was an inherent need and desire to bring all parts of the team closer together in order to get us working more efficiently both with one another and with our customers.\nUnfortunately, Jira became something of a punching bag due to its initial bugginess and it\u0026rsquo;s \u0026ldquo;not what I\u0026rsquo;m used to\u0026rdquo; faults. Rather than embrace the potential of the software, its additional features were (and often still are) seen as unnecessary complications to what should be a simple product. As we continue to develop not only the features of the platform, but also the way we use it to organise projects, I am hoping to see that attitude melt away. In the meantime, I\u0026rsquo;m trying to bring all of ServiceDesk\u0026rsquo;s functionality on board in order to tide everybody over until we can get around to embracing the new.\nLeading by example This whole experience has highlighted something for me: leading by example is of paramount importance. Not only do management need to be enthusiastically using the platform from the get-go in order to get the rest of us excited about it, it is also important that IT practitioners do not shy away from new technology lest they pass this apathy on to the users. After all, if the people working in IT can\u0026rsquo;t even get excited about it, why should anybody else?\nWhile I am a huge lover of FLOSS and the GPL, I am open to embracing proprietary platforms for work as long as the culture of the team evolves with them.\n","permalink":"https://sporiff.github.io/posts/acclimitising-to-new-tech/","tags":["tech","jira","work"],"title":"Acclimitising to new tech"},{"categories":["blog"],"contents":"A few years ago, I\u0026rsquo;d have been shocked to see the sort of vulnerabilities I see announced these days once a year, let alone once a month. But the rise of cybersecurity as a respected industry has led to the big companies such as Microsoft, Apple, Canonical, etc. pulling up their socks and crushing vulnerabilities in a timely manner. This is all great, except for those of us whose job it is to deploy these fixes.\nThe big news out of this week\u0026rsquo;s Patch Tuesday was undoubtedly the announcement of yet another flaw with Intel\u0026rsquo;s x86 architecture. Following on from the disastrous Meltdown and Spectre, the latest vulnerability allows applications to hijack code and read memory due to a mistake in the implementation of kernel-level code. Nota bene, I am not a programmer or a chip expert, this is just what I\u0026rsquo;ve gleaned from El Reg\u0026rsquo;s far better explanation of the issue. What is more interesting to me is that this is not a flaw with x86 per se, but rather \u0026ldquo;the error appears to be due to developer interpretation of existing documentation.\u0026rdquo;\nBad documentation is bad If an individual reads a set of instructions and manages stumble along the way, that\u0026rsquo;s understandable and usually rectifiable. However, if everybody reading the instructions fails in the same way, the instructions are bad. End of story. This cannot be passed off as being developer error if the documentation was misleading or unclear. It can be presumed that Microsoft, Google, Apple, and companies/communities working on *BSD and Linux are pretty smart cookies indeed. I feel it is unlikely that all of them could have failed in the same way without being misled.\nSo what does this mean? I would personally be taking a good long look at Intel after the disastrous year they\u0026rsquo;ve been having and wondering whether or not we had made a huge mistake entrusting our standards and architectures to a company so driven by quick wins and marketing that they will happily sacrifice security and the long-term health of the industry. As their recent \u0026ldquo;meltdown\u0026rdquo; has shown us, they have been a company obsessed with being able to claim the fastest and bestest at the cost of sane design. And let\u0026rsquo;s face it: these chips aren\u0026rsquo;t really getting that much better.\nIt seems that Apple\u0026rsquo;s recent decision to move away from Intel architecture may not be such a crazy idea. Certainly, Intel is faster now, but it is only a matter of time and investment before others surpass them. Certainly, they\u0026rsquo;ve not made themselves many friends this year. This month\u0026rsquo;s vuln is naught more than another eye-roll for sysadmins and users at this point, a fault for which developers are being unfairly blamed.\nAnything else? Flash Player, of course, has yet another high vulnerability announcement. Time to get your Adobe hat on and grumble all the way to deployment on this one. Or better yet, get rid of it entirely and see how many people actually complain. Most of the rest of the serious stuff (17/21 critical) seems to be browser vulnerabilities. Par for the course…\n","permalink":"https://sporiff.github.io/posts/intel-fail/","tags":["tech","intel","security","meltdown","spectre"],"title":"Another day, another Intel fail"},{"categories":["blog"],"contents":"AntiVirus is a necessary evil. With the world more connected than ever before, every device needs protection and tools to allow administrators oversight. Currently, the AV I work with is Sophos' cloud-based solution for Windows, Mac, and Linux. When I was working on the frontline, I quickly became aware that Sophos did not deploy during imaging as it had initially done, nor could it easily be pushed out via SCCM. We spent a long time going around to each freshly imaged machine and loading Sophos on, rebooting the machine, and logging tickets to have its policy applied. This did not sit right with me, so upon my move to the systems team I decided to have a crack at simplifying the process.\nBuild deployment The issue, I soon discovered, was not one of packaging or deployment, but rather time. Sophos' installer goes out of date every 30 days, at which point a new executable needs to be downloaded for any new deployments. Understandably, this was not a practical solution for our application manager. Given my GNU/Linux background, the solution seemed straightforward enough: write a script to handle download and installation as one does with the Linux client. However, while this was easily facilitated by PowerShell\u0026rsquo;s Invoke-WebRequest cmdlet on Windows 10, Windows 7\u0026rsquo;s older (v2) Powershell had no such functionality. So we were faced with a dilemma: upgrade PowerShell or find a way to script the download and install in an older environment.\nThe first option seemed the best to both of us, as more PowerShell features for technicians on a machine could be useful in the long run. However, we encountered endless issues trying to deploy the upgrade packages in the OSD, so eventually I sat down to work out the script in .NET. The result was a much less clean but still functional PowerShell script.\nBoth of these scripts can be found on my GitHub.\nProtecting macOS macOS is somewhat more familiar to me than Windows due to its Unix base, but I was not yet used to working with DeployStudio to image macOS devices. I decided that if I could automate deployment for Windows, the same must be easy enough on macOS. Certainly, the download command curl is a lot more familiar to me than Invoke-WebRequest, however the installation of Sophos from a .zip required many more steps than simply executing a .exe like in Windows. More importantly, I came across another issue.\nSophos had reported a change to their installer which caused it to conflict with the default permissions set by DeployStudio during imaging. For this reason, it was necessary to edit the workflow thus: place the installer script in as a delayed script to run after first boot, then place a chmod script at the end of the sequence to run as the last step of the initial build process. Lo and behold, a working Sophos install.\nBoth of these scripts can be found in this GitHub repo.\nClearing false positives This one may be seen by some as a bit of a nasty hack, but unfortunately Sophos – like any software – is imperfect. By default, the AV will quarantine or clean up PUAs and viruses from a host machine and report the status of this operation in the console, usually with a yellow exclamation triangle for medium status or a red exclamation circle for bad status. Usually, manually clearing out these problems is sufficient to repair the status, but sometimes the Sophos Health service gets, well, stuck.\nClearing this one is a little messy, as I say, but it\u0026rsquo;s quite simple. First, the technician should verify that the threat has indeed been cleared. Then, tamper protection should be disabled on the affected machine. This is to allow the technician to stop the Sophos Health service, rename the stuck database, and re-enable the service. Tamper protection should then be enabled and the machine reset. Voilà! A nice green tick in Sophos Central.\nGoing forward Working with Sophos is something of an adventure. Due to the fact that it is cloud-based, it is always evolving. Sometimes, the changes are welcome – like a long-awaited fix for a problem – but other times they can present new challenges to which you will quickly need to adapt your environment. With a little scripting, Sophos management for endpoints becomes a lot easier.\n","permalink":"https://sporiff.github.io/posts/deploying-sophos/","tags":["sophos","av","sysadmin","tech","antivirus","sccm","azure","intune"],"title":"Deploying Sophos in a production environment"},{"categories":["rants"],"contents":"I\u0026rsquo;m a systems administrator by trade (or as I often find myself writing, a “systemd administrator” since that cancerous piece of bloatware consumes most of my troubleshooting life). It is, therefore, perhaps unsurprising that I like having control over the devices in my home. I refuse to use Apple devices apart from the one I have to use for work, I allow only GNU/Linux devices to be used on our home WiFi, and while my rooted Android phone is still somewhat lacking in end-user control, I plan to replace it with the Librem 5 when it launches.\nIt would be terribly naïve of me to presume that other people were so preoccupied with device control as myself. My partner, for example, is the sort of person who simply wants her computer to get out of her way when she’s using it. The slightest hint of maintenance or troubleshooting is enough to make her throw the machine down in anger and find any excuse not to continue with what she was doing. I believe that a lot of people are like this, and it is what has given rise to this wave of “easy tech”, or technology with which the user has little interaction and no ability to troubleshoot. This is a terrible thing.\nNo access? No answers This little rant of mine was inspired in part by this article about Amazon’s Alexa and her recent spout of the giggles. For those of you who are not members and therefore cannot get past the paywalls, here is the part that piqued my interest:\n You’ll just have to take [Amazon’s] explanation of misheard commands at face value: None of the processing is done client-side, there is no way for third parties to look at how Alexa devices really work, to poke around in the guts and discover causes and effects.\n ~ Brian Feldman\nEnd users seem to be infatuated with devices like Alexa and Google’s Home devices. My aunt has not only got one for herself but has also inflicted one on my wholly disinterested grandmother. Quite literally, everybody and their grandma seems to have one. It’s become one of those “must-have” gadgets for which everybody clambers on Cyber Monday and Black Friday and you can see why: it allows users to interact with services they already know and love but with an even greater degree of laziness than a laptop or smartphone already affords.\nThere’s something very sci-fi about the idea of walking into your home and being able to turn things on and off with a simple command, about being able to talk to your very own robot butler about today’s headlines and weather. People are excited by future things, and that is why Amazon’s Alexa and her ability to order you food with a simple grunt is so appealing to so many.\nBut Alexa and her ilk are also sci-fi in a much darker way. I’m not talking about the fact that they suck up data like a sentient drug-addicted vacuum snorts coke at a 70s night. I’m not even talking about the notion that these devices might somehow become intelligent and launch a Skynet-style attack on humanity. My problem is far more practical: I cannot fix it when it breaks.\n Amazon produced a statement claiming that the problem was the Echo devices incorrectly hearing themselves being commanded to laugh. “We are disabling the short utterance ‘Alexa, laugh.’ We are also changing Alexa’s response from simply laughter to ‘Sure, I can laugh’ followed by laughter,” the company said in a statement.\n ~ Brian Feldman\nAmazon’s reported fix for the problem seems reasonable enough. Alexa is simply mishearing you, so we’ll change that command and make it so that she issues a warning when she’s about to laugh like a child in an 80s horror movie. For most users, this will satisfy. But I have to ask the question: how do I know that this is what the problem was? We now know that Alexa was reportedly mishearing commands such as “lamp” and “light” as “laugh”. And if my command history did not include any of these, or I simply did not have any connected devices like these, and it still laughed then how could I go about finding the solution?\nWhen my computers break (which they frequently do, usually due to my actions) I have a fairly good shot at finding out what is wrong. Log files are easily accessed and read, Googling around usually brings me some sort of solution. At the end of the day, even on a Windows machine I can typically find some sort of log which will tell me exactly what happened just before the whole system fell over. Even if it’s a stupid piece of software, something will get logged. But with this new range of devices, so imprisoned by the companies which manufacture them, access is getting more and more limited for the end-user.\nTake, as an example, the iPhone. While Android definitely dominates in terms of numbers, iOS remains the gold standard for many. Now, I use one of these accursed things for work and it goes wrong constantly**. Usually, it’s something simple like WiFi dropping off every time I lock the phone then taking around 30 seconds to reconnect. If it were an Android, I could look in the system logs to find out why this was happening, but with iOS devices, I would need to own a Mac to do this. Vendor lock-in is becoming a severe issue, and devices like Google Home and Amazon Alexa, which do all of their processing server-side, present an even greater level of vendor-control and user-cuckoldry than ever before.\nInsist on control As I said before, I know many people for whom the idea of having more involvement with their computers and the maintenance of them is a horrifying thought. After all, computers are complicated and confusing, so the less you have to do with them the better, right? Absolutely not. Here’s my unpopular opinion upon which I’m certain I will expand at a later date: computers should be hard to use. These devices should require some investment of time to come to grips with, just like using any power tool should require the wearing of correct safety equipment and following best practices. If users are stupid we are going to be in a whole heap of trouble as computers start to take over our lives more and more.\nHome computers and smartphones are immensely powerful machines with which an individual can potentially do a great deal, but unfortunately most people have given away all control in favour of having a simpler time. But this attitude is incredibly wasteful. If you spend a certain amount of money on a device, you should get your money’s worth from it. Similarly, if you’re using any device for anything personal or professional, the only person who should be controlling it and interacting with your data is you.\nIf you have no idea how a machine works, and you have no way of fixing a problem or even finding out what the problem is or relates to, then you have wasted your money. You have bought yourself an expensive trash can and, in the end, it doesn’t even really belong to you. Sure, it sits in your house and you think it’s doing what you ask it to, but in reality you don’t know what its capabilities are nor do you know what its limitations are. That is a very sad fact.\n","permalink":"https://sporiff.github.io/posts/uncontrolled-devices/","tags":["tech","control","rant"],"title":"Uncontrolled devices: not even once"},{"categories":null,"contents":" Hi! My name\u0026rsquo;s Ciarán. I\u0026rsquo;m a technical writer and programmer working (and, soon, living) in Berlin.\nI\u0026rsquo;ve been working with computers since I was very young and have made the mistake of making them my career. I sometimes take notes on the frustrations I come up against and complain about various things that have displeased me on any given day. I change my mind a lot, though, so don\u0026rsquo;t be surprised if something I have previously praised ends up the subject of my ire.\nIn my spare time, I contribute to a few projects – notably Funkwhale. I also host a popular Funkwhale pod for podcasters, musicians, and listeners. If you like what I do, you can\n ","permalink":"https://sporiff.github.io/about/","tags":null,"title":"About me"}]